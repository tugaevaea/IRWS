{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import copy\n",
    "import numpy as np \n",
    "import itertools\n",
    "import more_itertools as mit\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer \n",
    "import string\n",
    "import re\n",
    "\n",
    "# load some queries for testing\n",
    "queries_text = pd.read_csv('nfcorpus/dev.all.queries', sep='\\t', names=['ID', 'TEXT'])\n",
    "queries_text.head(10)\n",
    "\n",
    "## Corpus Preprocessing\n",
    "\n",
    "def preprocess_corpus(data):\n",
    "    \n",
    "    ps = PorterStemmer()\n",
    "    \n",
    "    def stemSentence(sentence,ps):\n",
    "        token_words = word_tokenize(sentence)\n",
    "        stem_sentence = []\n",
    "        for word in token_words:\n",
    "            stem_sentence.append(ps.stem(word))\n",
    "            stem_sentence.append(\" \")\n",
    "        return \"\".join(stem_sentence)\n",
    "    \n",
    "    data['TEXT'] = data.apply(lambda x: stemSentence(x['TEXT'],ps), axis=1)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# enter your path of the corpus\n",
    "path = 'C:/Users/48668/Desktop/FSS2020/IR/project/nfcorpus/'\n",
    "\n",
    "# load corpus as preprocessed set of documents\n",
    "corpus = pd.read_csv(path + 'dev.docs', sep='\\t', names=['ID', 'TEXT'])\n",
    "\n",
    "# corpus preprocessing\n",
    "corpus = preprocess_corpus(corpus)\n",
    "# preview first rows\n",
    "corpus.head()\n",
    "\n",
    " ## Query Preprocessing\n",
    "\n",
    "def preprocess_queries(corpus, queries):\n",
    "    \n",
    "    def remove_punctuations(text): # remove punctuation\n",
    "        for punctuation in string.punctuation:\n",
    "            text = text.replace(punctuation, '')\n",
    "        return text\n",
    "\n",
    "    def remove_numbers(text): # remove numbers\n",
    "        return re.sub('[0-9]+', '', text)\n",
    "    \n",
    "    def lower_case(text): # lower case\n",
    "        text = text.lower()\n",
    "        return text \n",
    "    \n",
    "    def tokenize(text): #tokenize\n",
    "        return word_tokenize(text)\n",
    "    \n",
    "    stop = set(stopwords.words('english'))   \n",
    "    def stop_words(tokens): # stop words \n",
    "        filtered_words = []\n",
    "        for word in tokens:\n",
    "            if word not in stop:\n",
    "                filtered_words.append(word)\n",
    "        return filtered_words\n",
    "    \n",
    "    ps = PorterStemmer()\n",
    "    def stemming(tokens, ps): # stemming\n",
    "        return [ps.stem(w) for w in tokens] \n",
    "    \n",
    "    def corpus_vocab(corpus):\n",
    "        vocab = []\n",
    "        corpus_tokens = corpus.apply(lambda x: word_tokenize(x['TEXT']), axis=1)\n",
    "        for i, j in corpus_tokens.iteritems():\n",
    "            for token in j:\n",
    "                if token not in vocab:\n",
    "                    vocab.append(token)        \n",
    "        return vocab\n",
    "    \n",
    "    v = corpus_vocab(corpus)    \n",
    "    def filter_query(tokens):\n",
    "        t = []\n",
    "        for token in tokens:\n",
    "            if token in v:\n",
    "                t.append(token)\n",
    "        return t\n",
    "    \n",
    "    # apply functions\n",
    "    queries['TEXT'] = queries.apply(lambda x: remove_punctuations(x['TEXT']), axis=1)\n",
    "    queries['TEXT'] = queries.apply(lambda x: remove_numbers(x['TEXT']), axis=1)\n",
    "    queries['TEXT'] = queries.apply(lambda x: lower_case(x['TEXT']), axis=1)\n",
    "    queries['TEXT'] = queries.apply(lambda x: tokenize(x['TEXT']), axis=1)\n",
    "    queries['TEXT'] = queries.apply(lambda x: stop_words(x['TEXT']), axis=1)\n",
    "    queries['TEXT'] = queries.apply(lambda x: stemming(x['TEXT'],ps), axis=1)\n",
    "    queries['TEXT'] = queries.apply(lambda x: filter_query(x['TEXT']), axis=1)\n",
    "    \n",
    "    return queries\n",
    "\n",
    "# load some queries for testing\n",
    "queries_text = pd.read_csv('nfcorpus/dev.all.queries', sep='\\t', names=['ID', 'TEXT'])\n",
    "\n",
    "queries_text = preprocess_queries(corpus, queries_text)\n",
    "queries_text.head(10)\n",
    "\n",
    "## Term frequency\n",
    "\n",
    "# Term frequency\n",
    "def tf(corpus, column_name):\n",
    "    \n",
    "    def tokenize(string):\n",
    "        return string.split()\n",
    "    \n",
    "    def tf_string(string): \n",
    "        # create bag of words from the string\n",
    "        bow = tokenize(string)\n",
    "    \n",
    "        tf_dict = {}\n",
    "        for word in bow:\n",
    "            if word in tf_dict:\n",
    "                tf_dict[word] += 1\n",
    "            else:\n",
    "                tf_dict[word] = 1\n",
    "            \n",
    "        for word in tf_dict:\n",
    "            tf_dict[word] = tf_dict[word]/len(bow)### ??\n",
    "    \n",
    "        return tf_dict\n",
    "    \n",
    "    # call our function on every doc and store all these tf dictionaries. \n",
    "    tf_dict = {}\n",
    "    for index, row in corpus.iterrows():\n",
    "        doc_dict = tf_string(row[column_name])\n",
    "        tf_dict[index] = doc_dict\n",
    "            \n",
    "    return tf_dict\n",
    "\n",
    "# Inversed document frequency\n",
    "\n",
    "# Inversed document frequency\n",
    "def idf(corpus, tf_dict):\n",
    "    \n",
    "    # nomber of documents in corpus\n",
    "    no_of_docs = len(corpus.index)\n",
    "    \n",
    "    # term - key, number of docs term occured in\n",
    "    def count_occurances(tf_dict):\n",
    "        count_dict = {}\n",
    "        for key in tf_dict:\n",
    "            for key in tf_dict[key]:\n",
    "                if key in count_dict:\n",
    "                    count_dict[key] += 1\n",
    "                else:\n",
    "                    count_dict[key] = 1\n",
    "        return count_dict\n",
    "\n",
    "    idf_dict = {}\n",
    "    \n",
    "    count_dict = count_occurances(tf_dict)\n",
    "    \n",
    "    for key in count_dict:\n",
    "        idf_dict[key] = math.log(no_of_docs/count_dict[key])\n",
    "    \n",
    "    return idf_dict\n",
    "\n",
    "## TF-IDF\n",
    "\n",
    "# TF-IDF\n",
    "def tf_idf(tf_dict, idf_dict):   \n",
    "    tf_idf_dict = copy.deepcopy(tf_dict)\n",
    "    for doc, value in tf_idf_dict.items():\n",
    "        for word, value in tf_idf_dict[doc].items():\n",
    "            tf_idf_dict[doc][word] = value * idf_dict[word]\n",
    "    return tf_idf_dict\n",
    "\n",
    "## Cosine similarity\n",
    "\n",
    "# Convert tf_idf_dict to matrix\n",
    "def tf_idf_to_matrix(tf_idf_dict):\n",
    "    tf_idf_matrix = pd.DataFrame.from_dict(tf_idf_dict, \n",
    "                                           orient = 'index').fillna(0) # if word does not appear in doc we change NaN to\n",
    "    return tf_idf_matrix.sort_index()\n",
    "\n",
    "# Cosine similarity\n",
    "def cosine_similarity(v1, v2):\n",
    "    \n",
    "    def vector_magnitude(v):\n",
    "        return np.linalg.norm(v)\n",
    "    \n",
    "    def dot_product(v1, v2):\n",
    "        return np.dot(v1,v2)\n",
    "    \n",
    "    return dot_product(v1, v2)/ (vector_magnitude(v1) * vector_magnitude(v2))\n",
    "\n",
    "## Inverted indexing\n",
    "\n",
    "# Inverted index\n",
    "def inverted_index(tf_dict):\n",
    "    ii_dict = {}\n",
    "    for doc in tf_dict:\n",
    "        for term in tf_dict[doc]:            \n",
    "            if term in ii_dict:\n",
    "                ii_dict[term].append(doc)\n",
    "            else:           \n",
    "                ii_dict[term] = list()\n",
    "                ii_dict[term].append(doc)\n",
    "    return ii_dict\n",
    "\n",
    "## Tiered indexing\n",
    "\n",
    "# Tiered index\n",
    "def tiered_index(corpus, chunks):\n",
    "    \n",
    "    print('Function is tested on term \\'human\\'. It performs following steps:')\n",
    "    \n",
    "    tf_dict = tf(corpus, 'TEXT')\n",
    "    \n",
    "    def tf_inverted_index(tf_dict):\n",
    "        tf_ii_dict = {}\n",
    "        for doc in tf_dict:\n",
    "            for term in tf_dict[doc]:\n",
    "                if term not in tf_ii_dict:\n",
    "                    inner_dict = {}\n",
    "                    tf_ii_dict[term] = inner_dict\n",
    "                    inner_dict[doc] = tf_dict[doc][term]\n",
    "                else:\n",
    "                    tf_ii_dict[term][doc] = tf_dict[doc][term]\n",
    "        return tf_ii_dict\n",
    "    \n",
    "    tf_ii_dict = tf_inverted_index(tf_dict)\n",
    "    #print(\"\\nInverted index:\")\n",
    "    #print(tf_ii_dict[\"human\"])\n",
    "    \n",
    "    def sort_dict(tf_ii_dict):\n",
    "        for doc in tf_ii_dict:\n",
    "             tf_ii_dict[doc] = {k: v for k, v in sorted(tf_ii_dict[doc].items(), \n",
    "                                                        key=lambda item: item[1], reverse=True)} #explain\n",
    "        return tf_ii_dict\n",
    "    \n",
    "    \n",
    "    tf_ii_dict_sorted = sort_dict(tf_ii_dict)\n",
    "    #print(\"\\nSorted inverted index by tf(term, doc):\")\n",
    "    #print(tf_ii_dict_sorted[\"human\"])\n",
    "    \n",
    "    def transform_dict(tf_ii_dict_sorted):\n",
    "        new = {}\n",
    "        for k,v in tf_ii_dict_sorted.items():\n",
    "            new[k] = list(v)\n",
    "        return new\n",
    "    \n",
    "    transformed = transform_dict(tf_ii_dict_sorted)\n",
    "    #print(\"\\nSorted inverted index without tf(term,doc) values:\")\n",
    "    #print(transformed[\"human\"])\n",
    "    \n",
    "    def chunk_list(lst, chunks):\n",
    "        return [list(x) for x in mit.divide(chunks, lst)]\n",
    "    \n",
    "    def chunk_dict(transformed, chunks):\n",
    "        for term in transformed:\n",
    "            doc_chunks = chunk_list(transformed[term],chunks)\n",
    "            new = {}\n",
    "            for i in range(0,len(doc_chunks)):\n",
    "                new[i] = doc_chunks[i]\n",
    "            transformed[term] = new\n",
    "        return transformed\n",
    "    \n",
    "    tf_ii_dict_sorted = chunk_dict(transformed, chunks)\n",
    "    \n",
    "    def split_dict(tf_ii_dict_sorted, chunks):      \n",
    "        i = itertools.cycle(range(chunks))       \n",
    "        split = [dict() for _ in range(chunks)]\n",
    "        for k, v in tf_ii_dict_sorted.items():\n",
    "            split[next(i)][k] = v\n",
    "        return split\n",
    "    \n",
    "    #for doc in tf_ii_dict_sorted:\n",
    "    #    tf_ii_dict_sorted[doc] = split_dict(tf_ii_dict_sorted[doc],chunks)\n",
    "        \n",
    "    #print(\"\\nChunked inverted index:\")\n",
    "    #print(tf_ii_dict_sorted[\"human\"])\n",
    "    \n",
    "    def sort_chunks(tf_ii_dict_sorted):\n",
    "        for term, tier in tf_ii_dict_sorted.items():\n",
    "            for tier, lst in tf_ii_dict_sorted[term].items():\n",
    "                lst.sort()\n",
    "        return tf_ii_dict_sorted\n",
    "    \n",
    "    tf_ii_dict_sorted = sort_chunks(tf_ii_dict_sorted)\n",
    "    \n",
    "    #print(\"\\nChunked inverted index with sorted chunks (tiered index):\")\n",
    "    #print(tf_ii_dict_sorted[\"human\"])\n",
    "    return tf_ii_dict_sorted\n",
    "\n",
    "# Call the function on the corpus\n",
    "tiered_index_dict = tiered_index(corpus, 4) \n",
    "\n",
    "## Intersection algorithm\n",
    "\n",
    "def inter_one_list(p1,p2): #posting 1 list, posting 2 list\n",
    "    i=0\n",
    "    j=0\n",
    "    intersection = []\n",
    "    \n",
    "    while i < len(p1) and j < len(p2):\n",
    "        if p1[i] == p2[j]:\n",
    "            if i== 0 or p1[i] != p1[i-1]:\n",
    "                intersection.append(p1[i])\n",
    "            i += 1\n",
    "            j += 1           \n",
    "        elif p1[i] < p2[j]:\n",
    "            i += 1\n",
    "        else: # p[i] > p[j]\n",
    "            j += 1     \n",
    "    return intersection\n",
    "        \n",
    "def inter_n_lists(lst):\n",
    "    \n",
    "    rank_lst = sorted(lst, key = len)   \n",
    "    intersection = []\n",
    "    for i in range(len(rank_list)):\n",
    "        if len(rank_lst) <= 1:\n",
    "            intersection = rank_lst[0]\n",
    "        elif len(rank_lst) == 2:\n",
    "            intersection = inter_one_list(rank_lst[0], rank_lst[1])\n",
    "        else:\n",
    "            if i == 0:\n",
    "                intersection = inter_one_list(rank_lst[i], rank_lst[i+1])\n",
    "            elif i > 1:\n",
    "                intersection = inter_one_list(rank_lst[i], intersection)\n",
    "    return intersection\n",
    "\n",
    "\n",
    "## Retrieve postings for terms in query\n",
    "\n",
    "def retrieve_postings(corpus, queries, t_chunks):   \n",
    "    \n",
    "    tiered_index_dict = tiered_index(corpus, t_chunks)\n",
    "   \n",
    "    postings_tiers = []\n",
    "    \n",
    "    for i in range(0,len(queries)): \n",
    "        try:\n",
    "            dic = {}\n",
    "            dic[queries[i]] = tiered_index_dict[queries[i]]\n",
    "            postings_tiers.append(dic)           \n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    return postings_tiers\n",
    "\n",
    "# Retrieve postings for query tokens from tiered index\n",
    "query_postigs = retrieve_postings(corpus, queries_text.iloc[0][1], 1)\n",
    "query_postigs\n",
    "\n",
    "## Iterating through tieres\n",
    "\n",
    "# Retrieve postings for query tokens from tiered index\n",
    "query_postigs = retrieve_postings(queries_text.iloc[0][1])\n",
    "\n",
    "# Extract i-th tier from all postings\n",
    "\n",
    "def tieres(query_postigs, n):\n",
    "    l = []\n",
    "    for i in range(len(query_postigs)): # for each posting\n",
    "        d = query_postigs[i]\n",
    "        key = [key for key in d.keys()][0]\n",
    "        l.append(d[key][n]) # append i-th tier list\n",
    "\n",
    "    return l\n",
    "\n",
    "l = tieres(query_postigs,3)\n",
    " # change to for in in no of tieres\n",
    "print(len(l))\n",
    "        \n",
    "\n",
    "# Intersection on tieres \n",
    "for i in range(325):\n",
    "    retrieve_postings(queries_text.iloc[0][1])\n",
    "    #print(inter_n_lists(tieres(retrieve_postings(queries_text.iloc[i][1]),2)))\n",
    "\n",
    "    print(queries_text.iloc[i][1])\n",
    "    \n",
    "\n",
    "# Try custom query\n",
    "\n",
    "a = retrieve_postings('sugar blod')\n",
    "a = tieres(a,0)\n",
    "a = inter_n_lists(a)\n",
    "\n",
    "a\n",
    "\n",
    "# Vanilla approach vs. splitting queries (??) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
