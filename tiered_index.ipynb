{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import copy\n",
    "import numpy as np \n",
    "import itertools\n",
    "import more_itertools as mit\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer \n",
    "import string\n",
    "import re\n",
    "from IPython import get_ipython\n",
    "\n",
    "get_ipython().magic('run -i \"basic_retrieval.ipynb\"')\n",
    "get_ipython().magic('run -i \"preprocessing_corpus_queries.ipynb\"')\n",
    "get_ipython().magic('run -i \"evaluation.ipynb\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiered index structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiered index\n",
    "def tiered_index(corpus, chunks):\n",
    "    \n",
    "    print('Function is tested on term \\'ddt\\'. It performs following steps:')\n",
    "    \n",
    "    tf_dict = tf(corpus)\n",
    "    \n",
    "    def tf_inverted_index(tf_dict):\n",
    "        tf_ii_dict = {}\n",
    "        for doc in tf_dict:\n",
    "            for term in tf_dict[doc]:\n",
    "                if term not in tf_ii_dict:\n",
    "                    inner_dict = {}\n",
    "                    tf_ii_dict[term] = inner_dict\n",
    "                    inner_dict[doc] = tf_dict[doc][term]\n",
    "                else:\n",
    "                    tf_ii_dict[term][doc] = tf_dict[doc][term]\n",
    "        return tf_ii_dict\n",
    "    \n",
    "    tf_ii_dict = tf_inverted_index(tf_dict)\n",
    "    print(\"\\nInverted index:\")\n",
    "    print(tf_ii_dict[\"ddt\"])\n",
    "    \n",
    "    def sort_dict(tf_ii_dict):\n",
    "        for doc in tf_ii_dict:\n",
    "             tf_ii_dict[doc] = {k: v for k, v in sorted(tf_ii_dict[doc].items(), \n",
    "                                                        key=lambda item: item[1], reverse=True)}\n",
    "        return tf_ii_dict\n",
    "    \n",
    "    \n",
    "    tf_ii_dict_sorted = sort_dict(tf_ii_dict)\n",
    "    print(\"\\nSorted inverted index by tf(term, doc):\")\n",
    "    print(tf_ii_dict_sorted[\"ddt\"])\n",
    "    \n",
    "    def transform_dict(tf_ii_dict_sorted):\n",
    "        new = {}\n",
    "        for k,v in tf_ii_dict_sorted.items():\n",
    "            new[k] = list(v)\n",
    "        return new\n",
    "    \n",
    "    transformed = transform_dict(tf_ii_dict_sorted)\n",
    "    print(\"\\nSorted inverted index without tf(term,doc) values:\")\n",
    "    print(transformed[\"ddt\"])\n",
    "    \n",
    "    def chunk_list(lst, chunks):\n",
    "        return [list(x) for x in mit.divide(chunks, lst)]\n",
    "    \n",
    "    def chunk_dict(transformed, chunks):\n",
    "        for term in transformed:\n",
    "            doc_chunks = chunk_list(transformed[term],chunks)\n",
    "            new = {}\n",
    "            for i in range(0,len(doc_chunks)):\n",
    "                new[i] = doc_chunks[i]\n",
    "            transformed[term] = new\n",
    "        return transformed\n",
    "    \n",
    "    tf_ii_dict_sorted = chunk_dict(transformed, chunks)\n",
    "        \n",
    "    print(\"\\nChunked inverted index:\")\n",
    "    print(tf_ii_dict_sorted[\"ddt\"])\n",
    "    \n",
    "    def sort_chunks(tf_ii_dict_sorted):\n",
    "        for term, tier in tf_ii_dict_sorted.items():\n",
    "            for tier, lst in tf_ii_dict_sorted[term].items():\n",
    "                lst.sort()\n",
    "        return tf_ii_dict_sorted\n",
    "    \n",
    "    tf_ii_dict_sorted = sort_chunks(tf_ii_dict_sorted)\n",
    "    \n",
    "    print(\"\\nChunked inverted index with sorted chunks (tiered index):\")\n",
    "    print(tf_ii_dict_sorted[\"ddt\"])\n",
    "    return tf_ii_dict_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST Tiered index structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function is tested on term 'ddt'. It performs following steps:\n",
      "\n",
      "Inverted index:\n",
      "{135: 0.017045454545454544, 136: 0.007246376811594203, 1117: 0.005952380952380952, 1118: 0.014285714285714285, 1264: 0.008547008547008548, 1444: 0.006211180124223602, 2327: 0.018018018018018018, 3020: 0.0045045045045045045}\n",
      "\n",
      "Sorted inverted index by tf(term, doc):\n",
      "{2327: 0.018018018018018018, 135: 0.017045454545454544, 1118: 0.014285714285714285, 1264: 0.008547008547008548, 136: 0.007246376811594203, 1444: 0.006211180124223602, 1117: 0.005952380952380952, 3020: 0.0045045045045045045}\n",
      "\n",
      "Sorted inverted index without tf(term,doc) values:\n",
      "[2327, 135, 1118, 1264, 136, 1444, 1117, 3020]\n",
      "\n",
      "Chunked inverted index:\n",
      "{0: [2327, 135], 1: [1118, 1264], 2: [136, 1444], 3: [1117, 3020]}\n",
      "\n",
      "Chunked inverted index with sorted chunks (tiered index):\n",
      "{0: [135, 2327], 1: [1118, 1264], 2: [136, 1444], 3: [1117, 3020]}\n"
     ]
    }
   ],
   "source": [
    "#Call the function on the corpus\n",
    "tiered_index_dict = tiered_index(corpus, 4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intersection algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inter_one_list(p1,p2): #posting 1 list, posting 2 list\n",
    "    i=0\n",
    "    j=0\n",
    "    intersection = []\n",
    "    \n",
    "    while i < len(p1) and j < len(p2):\n",
    "        if p1[i] == p2[j]:\n",
    "            if i== 0 or p1[i] != p1[i-1]:\n",
    "                intersection.append(p1[i])\n",
    "            i += 1\n",
    "            j += 1           \n",
    "        elif p1[i] < p2[j]:\n",
    "            i += 1\n",
    "        else: # p[i] > p[j]\n",
    "            j += 1     \n",
    "    return intersection\n",
    "\n",
    "def inter_n_lists(lst):\n",
    "    \n",
    "    rank_lst = sorted(lst, key = len)   \n",
    "    \n",
    "    if len(rank_lst) == 0:\n",
    "        rank_lst = rank_lst\n",
    "\n",
    "    if len(rank_lst) <= 1 and len(rank_lst) > 0:\n",
    "        intersection = rank_lst[0]\n",
    "    \n",
    "    while len(rank_lst) > 1:\n",
    "        intersection = inter_one_list(rank_lst[0], rank_lst[1])\n",
    "        del rank_lst[:2]\n",
    "        rank_lst.append(intersection)\n",
    "        rank_lst = sorted(rank_lst, key = len)\n",
    "                \n",
    "    return rank_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiered index retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tired_index_retrieve(tiered_index_dict, queries, query_index, doc_vectors, idf_dict, tieres_no, top_k):\n",
    "    \n",
    "    query = queries['TEXT'][query_index]   \n",
    "    \n",
    "    def retrieve_postings(query): \n",
    "        postings = []\n",
    "        for i in range(0,len(query)): \n",
    "            try:\n",
    "                dic = {}\n",
    "                dic[query[i]] = tiered_index_dict[query[i]]\n",
    "                postings.append(dic)           \n",
    "            except KeyError:\n",
    "                pass\n",
    "        return postings\n",
    "\n",
    "    def get_tieres(postings, t):\n",
    "        tieres_lst = []\n",
    "        for i in range(len(postings)): \n",
    "            d = postings[i]\n",
    "            key = [key for key in d.keys()][0]  \n",
    "            element = d[key][t]\n",
    "            tieres_lst.append(element)\n",
    "        return tieres_lst\n",
    "    \n",
    "    if len(query) == 0:\n",
    "        #print(\"\\nQuery is empty!\")\n",
    "        pass\n",
    "    else:\n",
    "        #print(\"\\nQuery: \", query)\n",
    "        postings = retrieve_postings(query)\n",
    "    \n",
    "        t=-1\n",
    "        tieres = [[]] * len(postings)\n",
    "        common_docs = []\n",
    "    \n",
    "        while (len(common_docs) < top_k) and t+1 < tieres_no:\n",
    "        \n",
    "            next_tieres = get_tieres(postings, (t+1)) # get next tieres        \n",
    "        \n",
    "            for i in range(len(tieres)): # merge them with previous tieres\n",
    "                tieres[i] = tieres[i] + next_tieres[i] \n",
    "                tieres[i].sort() # we need to sort merged tieres!!!\n",
    "        \n",
    "            common_docs = inter_n_lists(tieres) # intersection\n",
    "\n",
    "            t+=1   \n",
    "    \n",
    "        q_vector = build_q_vector(query, doc_vectors, idf_dict)\n",
    "    \n",
    "        if len(common_docs[0]) < top_k:\n",
    "            #print(\"\\nNo documents find via tired index, basic retrieval performed.\")\n",
    "            return basic_retrieve(q_vector = q_vector,\n",
    "                      doc_vectors = doc_vectors, \n",
    "                      top_k = 5,\n",
    "                      idf_dict = idf_dict)\n",
    "    \n",
    "        else:\n",
    "            #print(\"\\nDocuments found via tired index\")\n",
    "            doc_vectors_cropped = doc_vectors[doc_vectors.index.isin(common_docs[0])]\n",
    "        \n",
    "            return basic_retrieve(q_vector = q_vector,\n",
    "                                  doc_vectors = doc_vectors_cropped, \n",
    "                                  top_k = top_k,\n",
    "                                  idf_dict = idf_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST Tiered index retrieval (3 tieres, find top 3 documents for first 10 queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load corpus \n",
    "corpus = pd.read_csv('nfcorpus/dev.docs', sep='\\t', names=['ID', 'TEXT'])\n",
    "\n",
    "# load queries (titles)\n",
    "queries = preprocess_queries(corpus, pd.read_csv('nfcorpus/dev.titles.queries', sep='\\t', names=['ID', 'TEXT']))\n",
    "\n",
    "# load up relevance for queries titles\n",
    "queries_relevance = pd.read_csv('nfcorpus/dev.2-1-0.qrel', sep='\\t', names=['QUERY_ID', '0', 'DOC_ID', 'RELEVANCE_LEVEL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query:  ['deep', 'food', 'may', 'cancer']\n",
      "\n",
      "No documents find via tired index, basic retrieval performed.\n",
      "            ID                                               TEXT\n",
      "1302  MED-2697  impaired endothelial function meal rich cookin...\n",
      "1965  MED-3703  association allergies cancer pubmed ncbi abstr...\n",
      "582   MED-1721  cancer incidence mortality relation body mass ...\n",
      "132   MED-1151  organic food consumption incidence cancer larg...\n",
      "1961  MED-3699  concordance world cancer research fund/america...\n",
      "\n",
      "Query:  ['ddt']\n",
      "\n",
      "Documents found via tired index\n",
      "        ID                                               TEXT\n",
      "0  MED-118  alkylphenols human milk relations dietary habi...\n",
      "2  MED-330  dietary phosphorus acutely impairs endothelial...\n",
      "5  MED-335  differences total vitro digestible phosphorus ...\n",
      "\n",
      "Query:  ['treat', 'diet']\n",
      "\n",
      "Documents found via tired index\n",
      "        ID                                               TEXT\n",
      "2  MED-330  dietary phosphorus acutely impairs endothelial...\n",
      "4  MED-334  differences total vitro digestible phosphorus ...\n",
      "0  MED-118  alkylphenols human milk relations dietary habi...\n",
      "\n",
      "Query is empty!\n",
      "None\n",
      "\n",
      "Query is empty!\n",
      "None\n",
      "\n",
      "Query:  ['dog']\n",
      "\n",
      "Documents found via tired index\n",
      "        ID                                               TEXT\n",
      "4  MED-334  differences total vitro digestible phosphorus ...\n",
      "8  MED-666  role surgery treatment mastalgia pubmed ncbi a...\n",
      "6  MED-398  grapefruit wine glass metabolic cardiovascular...\n",
      "\n",
      "Query:  ['dr']\n",
      "\n",
      "Documents found via tired index\n",
      "        ID                                               TEXT\n",
      "5  MED-335  differences total vitro digestible phosphorus ...\n",
      "0  MED-118  alkylphenols human milk relations dietary habi...\n",
      "2  MED-330  dietary phosphorus acutely impairs endothelial...\n",
      "\n",
      "Query:  ['dr']\n",
      "\n",
      "Documents found via tired index\n",
      "        ID                                               TEXT\n",
      "5  MED-335  differences total vitro digestible phosphorus ...\n",
      "0  MED-118  alkylphenols human milk relations dietary habi...\n",
      "2  MED-330  dietary phosphorus acutely impairs endothelial...\n",
      "\n",
      "Query is empty!\n",
      "None\n",
      "\n",
      "Query:  ['easter', 'island']\n",
      "\n",
      "No documents find via tired index, basic retrieval performed.\n",
      "            ID                                               TEXT\n",
      "1208  MED-2521  rapamycin ay num antifungal antibiotic taxonom...\n",
      "1558  MED-3135  cpg island tumor suppressor promoter methylati...\n",
      "460   MED-1571  ciguatera union island sw indian ocean epidemi...\n",
      "310   MED-1376  sociodemographic lifestyle statistics oldest p...\n",
      "2492  MED-4448  construction flavonoid database assessing inta...\n"
     ]
    }
   ],
   "source": [
    "# get needed arguments\n",
    "doc_vectors = build_doc_vectors(corpus)\n",
    "idf_dict = idf(corpus)\n",
    "no_tieres = 3\n",
    "tiered_index_dict = tiered_index(corpus,4)\n",
    "\n",
    "\n",
    "# retrieve\n",
    "for i in range(10):\n",
    "    test = tired_index_retrieve(tiered_index_dict = tiered_index_dict , \n",
    "                            queries = queries, \n",
    "                            query_index = i, \n",
    "                            doc_vectors = doc_vectors, \n",
    "                            idf_dict = idf_dict, \n",
    "                            tieres_no = no_tieres, \n",
    "                            top_k = 3)\n",
    "    print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve for this query relevant documents\n",
    "def true_relevant_docs(string_query):\n",
    "    query_row = (queries.loc[queries['TEXT'].isin([string_query])])\n",
    "    query_id = query_row.iloc[0][\"ID\"]\n",
    "    relevance_lvl = [1, 2]\n",
    "    return queries_relevance.loc[queries_relevance['QUERY_ID'].isin([query_id]) \n",
    "                                 & queries_relevance['RELEVANCE_LEVEL'].isin(relevance_lvl)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_single_retrieve(retrieved_df, relevant, docs, k=3, random_projections = False):\n",
    "    ## returns the triple (Precision, Average Precision, Normalized Discounted Cumulative Gain)\n",
    "    \n",
    "    if retrieved_df is None:\n",
    "        return 0, 0, 0\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        ids_retrieved = []\n",
    "        for i in range(len(retrieved_df)):\n",
    "            ids_retrieved.append(retrieved_df.iloc[i].ID)\n",
    "        ids_retrieved.sort()\n",
    "\n",
    "        ids_true_relevant = []\n",
    "        for i in range(len(relevant)):\n",
    "            ids_true_relevant.append(relevant.iloc[i].DOC_ID)\n",
    "        ids_true_relevant.sort()\n",
    "    \n",
    "        # count true positives and false positives\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        for i in ids_retrieved:\n",
    "            for j in ids_true_relevant:\n",
    "                if i == j:\n",
    "                    tp += 1\n",
    "                    break\n",
    "                else:\n",
    "                    if i < j:\n",
    "                        fp += 1\n",
    "                        break\n",
    "                    else:\n",
    "                        continue\n",
    "        if (tp == 0) & (fp == 0):\n",
    "            precision = 0\n",
    "        else:\n",
    "            precision = tp / (tp + fp)\n",
    "        # cannot calculate recall, since we predefined the number of retrieved documents => apriori algorithm cannot retrieve all documents\n",
    "\n",
    "        # then calculate Average precision across retrieved documents\n",
    "        ap = apk(ids_true_relevant, ids_retrieved)\n",
    "\n",
    "        # since we have graded relevance annotations, we can also calculate Normalized Discounted Cumulative Gain\n",
    "        list_of_ranks_of_retrieved_docs = []\n",
    "        for i in ids_retrieved:\n",
    "            if i in ids_true_relevant:\n",
    "                list_of_ranks_of_retrieved_docs.append(relevant.loc[relevant['DOC_ID'].isin([i])].RELEVANCE_LEVEL.iloc[0])\n",
    "            else:\n",
    "                list_of_ranks_of_retrieved_docs.append(0)\n",
    "\n",
    "        list_of_ranks_of_relevant_docs = []\n",
    "        for i in ids_true_relevant:\n",
    "            list_of_ranks_of_relevant_docs.append(relevant.loc[relevant['DOC_ID'].isin([i])].RELEVANCE_LEVEL.iloc[0])\n",
    "        list_of_ranks_of_relevant_docs.sort(reverse=True)\n",
    "\n",
    "        k = len(list_of_ranks_of_retrieved_docs)\n",
    "        list_of_ranks_of_relevant_docs = list_of_ranks_of_relevant_docs[:k]\n",
    "\n",
    "        return precision, ap, ndcg(list_of_ranks_of_relevant_docs, list_of_ranks_of_retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_evaluation(queries, \n",
    "                    tiered_index_dict, \n",
    "                    idf_dict, \n",
    "                    no_tieres,\n",
    "                    docs, \n",
    "                    k=3, \n",
    "                    random_projections = False):\n",
    "    \n",
    "    evaluation = queries.copy()\n",
    "    evaluation.insert(2, \"Precision\", 0)\n",
    "    evaluation.insert(3, \"Average Precision\", 0)\n",
    "    evaluation.insert(4, \"nDCG\", 0)\n",
    "    \n",
    "    for i in range(len(evaluation)):\n",
    "        \n",
    "        retrieved_df = tired_index_retrieve(tiered_index_dict = tiered_index_dict , \n",
    "                            queries = queries, \n",
    "                            query_index = i, \n",
    "                            doc_vectors = docs, \n",
    "                            idf_dict = idf_dict, \n",
    "                            tieres_no = no_tieres, \n",
    "                            top_k = k)\n",
    "        \n",
    "        relevant = true_relevant_docs(queries[\"TEXT\"][i])\n",
    "        \n",
    "        p, a, n = evaluate_single_retrieve(retrieved_df, relevant, doc_vectors, k=k, random_projections = False)\n",
    "        evaluation.loc[i, 'Precision'] = p\n",
    "        evaluation.loc[i, 'Average Precision'] = a\n",
    "        evaluation.loc[i, 'nDCG'] = n\n",
    "\n",
    "    print('Average precision across all queries = ' + str(evaluation['Precision'].mean()))\n",
    "    print('Mean Average Precision = ' + str(evaluation['Average Precision'].mean()))\n",
    "    print('Average nDCG = ' + str(evaluation['nDCG'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get needed arguments\n",
    "doc_vectors = build_doc_vectors(corpus)\n",
    "idf_dict = idf(corpus)\n",
    "no_tieres = 3\n",
    "tiered_index_dict = tiered_index(corpus,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision across all queries = 0.16471794871794884\n",
      "Mean Average Precision = 0.10835042735042735\n",
      "Average nDCG = 0.14093716742904813\n",
      "Wall time: 15min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "full_evaluation(queries = queries, \n",
    "                tiered_index_dict = tiered_index_dict, \n",
    "                idf_dict = idf_dict, \n",
    "                no_tieres = no_tieres ,\n",
    "                docs = doc_vectors, \n",
    "                k=3, \n",
    "                random_projections = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
