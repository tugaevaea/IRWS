{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import copy\n",
    "import numpy as np \n",
    "import itertools\n",
    "import more_itertools as mit\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer \n",
    "import string\n",
    "import re\n",
    "from IPython import get_ipython\n",
    "\n",
    "get_ipython().magic('run -i \"2_basic_retrieval.ipynb\"')\n",
    "get_ipython().magic('run -i \"1_preprocessing_corpus_queries.ipynb\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiered index structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiered index\n",
    "def tiered_index(corpus, chunks):\n",
    "    \n",
    "    #print('Function is tested on term \\'ddt\\'. It performs following steps:')\n",
    "    \n",
    "    tf_dict = tf(corpus)\n",
    "    \n",
    "    def tf_inverted_index(tf_dict):\n",
    "        tf_ii_dict = {}\n",
    "        for doc in tf_dict:\n",
    "            for term in tf_dict[doc]:\n",
    "                if term not in tf_ii_dict:\n",
    "                    inner_dict = {}\n",
    "                    tf_ii_dict[term] = inner_dict\n",
    "                    inner_dict[doc] = tf_dict[doc][term]\n",
    "                else:\n",
    "                    tf_ii_dict[term][doc] = tf_dict[doc][term]\n",
    "        return tf_ii_dict\n",
    "    \n",
    "    tf_ii_dict = tf_inverted_index(tf_dict)\n",
    "    #print(\"\\nInverted index:\")\n",
    "    #print(tf_ii_dict[\"ddt\"])\n",
    "    \n",
    "    def sort_dict(tf_ii_dict):\n",
    "        for doc in tf_ii_dict:\n",
    "             tf_ii_dict[doc] = {k: v for k, v in sorted(tf_ii_dict[doc].items(), \n",
    "                                                        key=lambda item: item[1], reverse=True)}\n",
    "        return tf_ii_dict\n",
    "    \n",
    "    \n",
    "    tf_ii_dict_sorted = sort_dict(tf_ii_dict)\n",
    "    #print(\"\\nSorted inverted index by tf(term, doc):\")\n",
    "    #print(tf_ii_dict_sorted[\"ddt\"])\n",
    "    \n",
    "    def transform_dict(tf_ii_dict_sorted):\n",
    "        new = {}\n",
    "        for k,v in tf_ii_dict_sorted.items():\n",
    "            new[k] = list(v)\n",
    "        return new\n",
    "    \n",
    "    transformed = transform_dict(tf_ii_dict_sorted)\n",
    "    #print(\"\\nSorted inverted index without tf(term,doc) values:\")\n",
    "    #print(transformed[\"ddt\"])\n",
    "    \n",
    "    def chunk_list(lst, chunks):\n",
    "        return [list(x) for x in mit.divide(chunks, lst)]\n",
    "    \n",
    "    def chunk_dict(transformed, chunks):\n",
    "        for term in transformed:\n",
    "            doc_chunks = chunk_list(transformed[term],chunks)\n",
    "            new = {}\n",
    "            for i in range(0,len(doc_chunks)):\n",
    "                new[i] = doc_chunks[i]\n",
    "            transformed[term] = new\n",
    "        return transformed\n",
    "    \n",
    "    tf_ii_dict_sorted = chunk_dict(transformed, chunks)\n",
    "        \n",
    "    #print(\"\\nChunked inverted index:\")\n",
    "    #print(tf_ii_dict_sorted[\"ddt\"])\n",
    "    \n",
    "    def sort_chunks(tf_ii_dict_sorted):\n",
    "        for term, tier in tf_ii_dict_sorted.items():\n",
    "            for tier, lst in tf_ii_dict_sorted[term].items():\n",
    "                lst.sort()\n",
    "        return tf_ii_dict_sorted\n",
    "    \n",
    "    tf_ii_dict_sorted = sort_chunks(tf_ii_dict_sorted)\n",
    "    \n",
    "    #print(\"\\nChunked inverted index with sorted chunks (tiered index):\")\n",
    "    #print(tf_ii_dict_sorted[\"ddt\"])\n",
    "    return tf_ii_dict_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST Tiered index structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call the function on the corpus\n",
    "tiered_index_dict = tiered_index(corpus, 4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intersection algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inter_one_list(p1,p2): #posting 1 list, posting 2 list\n",
    "    i=0\n",
    "    j=0\n",
    "    intersection = []\n",
    "    \n",
    "    while i < len(p1) and j < len(p2):\n",
    "        if p1[i] == p2[j]:\n",
    "            if i== 0 or p1[i] != p1[i-1]:\n",
    "                intersection.append(p1[i])\n",
    "            i += 1\n",
    "            j += 1           \n",
    "        elif p1[i] < p2[j]:\n",
    "            i += 1\n",
    "        else: # p[i] > p[j]\n",
    "            j += 1     \n",
    "    return intersection\n",
    "\n",
    "# first sort list in ascending order\n",
    "# take first two elements and produce output\n",
    "# delete these two merged lists from super list\n",
    "# append result of merge\n",
    "def inter_n_lists(lst):\n",
    "    \n",
    "    rank_lst = sorted(lst, key = len)   \n",
    "    \n",
    "    if len(rank_lst) == 0:\n",
    "        rank_lst = rank_lst\n",
    "\n",
    "    if len(rank_lst) <= 1 and len(rank_lst) > 0:\n",
    "        intersection = rank_lst[0]\n",
    "    \n",
    "    while len(rank_lst) > 1:\n",
    "        intersection = inter_one_list(rank_lst[0], rank_lst[1])\n",
    "        del rank_lst[:2]\n",
    "        rank_lst.append(intersection)\n",
    "        rank_lst = sorted(rank_lst, key = len)\n",
    "                \n",
    "    return rank_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiered index retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tired_index_retrieve(tiered_index_dict, queries, query_index, doc_vectors, idf_dict, tieres_no, top_k):\n",
    "    \n",
    "    query = queries['TEXT'][query_index]   \n",
    "    \n",
    "    def retrieve_postings(query): \n",
    "        postings = []\n",
    "        for i in range(0,len(query)): \n",
    "            try:\n",
    "                dic = {}\n",
    "                dic[query[i]] = tiered_index_dict[query[i]]\n",
    "                postings.append(dic)           \n",
    "            except KeyError:\n",
    "                pass\n",
    "        return postings\n",
    "\n",
    "    def get_tieres(postings, t):\n",
    "        tieres_lst = []\n",
    "        for i in range(len(postings)): \n",
    "            d = postings[i]\n",
    "            key = [key for key in d.keys()][0]  \n",
    "            element = d[key][t]\n",
    "            tieres_lst.append(element)\n",
    "        return tieres_lst\n",
    "    \n",
    "    if len(query) == 0:\n",
    "        print(\"\\nQuery is empty!\")\n",
    "    else:\n",
    "        print(\"\\nQuery: \", query)\n",
    "        postings = retrieve_postings(query)\n",
    "    \n",
    "        t=-1\n",
    "        tieres = [[]] * len(postings)\n",
    "        common_docs = []\n",
    "    \n",
    "        while (len(common_docs) < top_k) and t+1 < tieres_no:\n",
    "        \n",
    "            next_tieres = get_tieres(postings, (t+1)) # get next tieres        \n",
    "        \n",
    "            for i in range(len(tieres)): # merge them with previous tieres\n",
    "                tieres[i] = tieres[i] + next_tieres[i] \n",
    "                tieres[i].sort() # we need to sort merged tieres!!!\n",
    "        \n",
    "            common_docs = inter_n_lists(tieres) # intersection\n",
    "\n",
    "            t+=1   \n",
    "     \n",
    "        # Calculate cosine similariy between query vector and outputed documents (common_docs)\n",
    "    \n",
    "        # if no common (or not enough) documents were found perform basic retrieval\n",
    "    \n",
    "        q_vector = build_q_vector(query, doc_vectors, idf_dict)\n",
    "    \n",
    "        if len(common_docs[0]) < top_k:\n",
    "            print(\"\\nNo documents find via tired index, basic retrieval performed.\")\n",
    "            return basic_retrieve(q_vector = q_vector,\n",
    "                      doc_vectors = doc_vectors, \n",
    "                      top_k = 5,\n",
    "                      idf_dict = idf_dict)\n",
    "    \n",
    "        else:\n",
    "            print(\"\\nDocuments found via tired index\")\n",
    "            doc_vectors_cropped = doc_vectors[doc_vectors.index.isin(common_docs[0])]\n",
    "        \n",
    "            return basic_retrieve(q_vector = q_vector,\n",
    "                                  doc_vectors = doc_vectors_cropped, \n",
    "                                  top_k = top_k,\n",
    "                                  idf_dict = idf_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST Tiered index retrieval (3 tieres, find top 3 documents for first 3 queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load corpus \n",
    "corpus = pd.read_csv('nfcorpus/dev.docs', sep='\\t', names=['ID', 'TEXT'])\n",
    "\n",
    "# load queries (titles)\n",
    "queries = preprocess_queries(corpus, pd.read_csv('nfcorpus/dev.titles.queries', sep='\\t', names=['ID', 'TEXT']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query:  ['deep', 'food', 'may', 'cancer']\n",
      "\n",
      "No documents find via tired index, basic retrieval performed.\n",
      "            ID                                               TEXT\n",
      "1302  MED-2697  impaired endothelial function meal rich cookin...\n",
      "1965  MED-3703  association allergies cancer pubmed ncbi abstr...\n",
      "582   MED-1721  cancer incidence mortality relation body mass ...\n",
      "132   MED-1151  organic food consumption incidence cancer larg...\n",
      "1961  MED-3699  concordance world cancer research fund/america...\n",
      "\n",
      "Query:  ['ddt']\n",
      "\n",
      "Documents found via tired index\n",
      "        ID                                               TEXT\n",
      "0  MED-118  alkylphenols human milk relations dietary habi...\n",
      "2  MED-330  dietary phosphorus acutely impairs endothelial...\n",
      "5  MED-335  differences total vitro digestible phosphorus ...\n",
      "\n",
      "Query:  ['treat', 'diet']\n"
     ]
    }
   ],
   "source": [
    "# get needed arguments\n",
    "doc_vectors = build_doc_vectors(corpus)\n",
    "idf_dict = idf(corpus)\n",
    "no_tieres = 3\n",
    "tiered_index_dict = tiered_index(corpus,4)\n",
    "\n",
    "\n",
    "# retrieve\n",
    "for i in range(10):\n",
    "    test = tired_index_retrieve(tiered_index_dict = tiered_index_dict , \n",
    "                            queries = queries, \n",
    "                            query_index = i, \n",
    "                            doc_vectors = doc_vectors, \n",
    "                            idf_dict = idf_dict, \n",
    "                            tieres_no = no_tieres, \n",
    "                            top_k = 3)\n",
    "    print(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
