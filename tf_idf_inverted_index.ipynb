{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import copy\n",
    "import numpy as np \n",
    "import itertools\n",
    "import more_itertools as mit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MED-118</td>\n",
       "      <td>alkylphenols human milk relations dietary habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MED-329</td>\n",
       "      <td>phosphate vascular toxin pubmed ncbi abstract ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MED-330</td>\n",
       "      <td>dietary phosphorus acutely impairs endothelial...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MED-332</td>\n",
       "      <td>public health impact dietary phosphorus excess...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MED-334</td>\n",
       "      <td>differences total vitro digestible phosphorus ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID                                               TEXT\n",
       "0  MED-118  alkylphenols human milk relations dietary habi...\n",
       "1  MED-329  phosphate vascular toxin pubmed ncbi abstract ...\n",
       "2  MED-330  dietary phosphorus acutely impairs endothelial...\n",
       "3  MED-332  public health impact dietary phosphorus excess...\n",
       "4  MED-334  differences total vitro digestible phosphorus ..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# enter your path of the corpus\n",
    "path = 'C:/Users/48668/Desktop/FSS2020/IR/project/nfcorpus/'\n",
    "\n",
    "# load corpus as preprocessed set of documents\n",
    "corpus = pd.read_csv(path + 'dev.docs', sep='\\t', names=['ID', 'TEXT'])\n",
    "\n",
    "# preview first rows\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Term frequency\n",
    "def tf(corpus, column_name):\n",
    "    \n",
    "    def tokenize(string):\n",
    "        return string.split()\n",
    "    \n",
    "    def tf_string(string): \n",
    "        # create bag of words from the string\n",
    "        bow = tokenize(string)\n",
    "    \n",
    "        tf_dict = {}\n",
    "        for word in bow:\n",
    "            if word in tf_dict:\n",
    "                tf_dict[word] += 1\n",
    "            else:\n",
    "                tf_dict[word] = 1\n",
    "            \n",
    "        for word in tf_dict:\n",
    "            tf_dict[word] = tf_dict[word]/len(bow)### ??\n",
    "    \n",
    "        return tf_dict\n",
    "    \n",
    "    # call our function on every doc and store all these tf dictionaries. \n",
    "    tf_dict = {}\n",
    "    for index, row in corpus.iterrows():\n",
    "        doc_dict = tf_string(row[column_name])\n",
    "        tf_dict[index] = doc_dict\n",
    "            \n",
    "    return tf_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inversed document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inversed document frequency\n",
    "def idf(corpus, tf_dict):\n",
    "    \n",
    "    # nomber of documents in corpus\n",
    "    no_of_docs = len(corpus.index)\n",
    "    \n",
    "    # term - key, number of docs term occured in\n",
    "    def count_occurances(tf_dict):\n",
    "        count_dict = {}\n",
    "        for key in tf_dict:\n",
    "            for key in tf_dict[key]:\n",
    "                if key in count_dict:\n",
    "                    count_dict[key] += 1\n",
    "                else:\n",
    "                    count_dict[key] = 1\n",
    "        return count_dict\n",
    "\n",
    "    idf_dict = {}\n",
    "    \n",
    "    count_dict = count_occurances(tf_dict)\n",
    "    \n",
    "    for key in count_dict:\n",
    "        idf_dict[key] = math.log(no_of_docs/count_dict[key])\n",
    "    \n",
    "    return idf_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "def tf_idf(tf_dict, idf_dict):      \n",
    "    tf_idf_dict = copy.deepcopy(tf_dict)\n",
    "    for doc, value in tf_idf_dict.items():\n",
    "        for word, value in tf_idf_dict[doc].items():\n",
    "            tf_idf_dict[doc][word] = value * idf_dict[word]\n",
    "    return tf_idf_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tf_idf_dict to matrix\n",
    "def tf_idf_to_matrix(tf_idf_dict):\n",
    "    tf_idf_matrix = pd.DataFrame.from_dict(tf_idf_dict, \n",
    "                                           orient = 'index').fillna(0) # if word does not appear in doc we change NaN to\n",
    "    return tf_idf_matrix.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity\n",
    "def cosine_similarity(v1, v2):\n",
    "    \n",
    "    def vector_magnitude(v):\n",
    "        return np.linalg.norm(v)\n",
    "    \n",
    "    def dot_product(v1, v2):\n",
    "        return np.dot(v1,v2)\n",
    "    \n",
    "    return dot_product(v1, v2)/ (vector_magnitude(v1) * vector_magnitude(v2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverted index\n",
    "def inverted_index(tf_dict):\n",
    "    ii_dict = {}\n",
    "    for doc in tf_dict:\n",
    "        for term in tf_dict[doc]:            \n",
    "            if term in ii_dict:\n",
    "                ii_dict[term].append(doc)\n",
    "            else:           \n",
    "                ii_dict[term] = list()\n",
    "                ii_dict[term].append(doc)\n",
    "    return ii_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiered indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiered index\n",
    "def tiered_index(corpus, chunks):\n",
    "    \n",
    "    print('Function is tested on term \\'human\\'. It performs following steps:')\n",
    "    \n",
    "    tf_dict = tf(corpus, 'TEXT')\n",
    "    \n",
    "    def tf_inverted_index(tf_dict):\n",
    "        tf_ii_dict = {}\n",
    "        for doc in tf_dict:\n",
    "            for term in tf_dict[doc]:\n",
    "                if term not in tf_ii_dict:\n",
    "                    inner_dict = {}\n",
    "                    tf_ii_dict[term] = inner_dict\n",
    "                    inner_dict[doc] = tf_dict[doc][term]\n",
    "                else:\n",
    "                    tf_ii_dict[term][doc] = tf_dict[doc][term]\n",
    "        return tf_ii_dict\n",
    "    \n",
    "    tf_ii_dict = tf_inverted_index(tf_dict)\n",
    "    print(\"\\nInverted index:\")\n",
    "    print(tf_ii_dict[\"diary\"])\n",
    "    \n",
    "    def sort_dict(tf_ii_dict):\n",
    "        for doc in tf_ii_dict:\n",
    "             tf_ii_dict[doc] = {k: v for k, v in sorted(tf_ii_dict[doc].items(), \n",
    "                                                        key=lambda item: item[1], reverse=True)} #explain\n",
    "        return tf_ii_dict\n",
    "    \n",
    "    \n",
    "    tf_ii_dict_sorted = sort_dict(tf_ii_dict)\n",
    "    print(\"\\nSorted inverted index by tf(term, doc):\")\n",
    "    print(tf_ii_dict_sorted[\"diary\"])\n",
    "    \n",
    "    def transform_dict(tf_ii_dict_sorted):\n",
    "        new = {}\n",
    "        for k,v in tf_ii_dict_sorted.items():\n",
    "            new[k] = list(v)\n",
    "        return new\n",
    "    \n",
    "    transformed = transform_dict(tf_ii_dict_sorted)\n",
    "    print(\"\\nSorted inverted index without tf(term,doc) values:\")\n",
    "    print(transformed[\"diary\"])\n",
    "    \n",
    "    def chunk_list(lst, chunks):\n",
    "        return [list(x) for x in mit.divide(chunks, lst)]\n",
    "    \n",
    "    def chunk_dict(transformed, chunks):\n",
    "        for term in transformed:\n",
    "            doc_chunks = chunk_list(transformed[term],chunks)\n",
    "            new = {}\n",
    "            for i in range(0,len(doc_chunks)):\n",
    "                new[i] = doc_chunks[i]\n",
    "            transformed[term] = new\n",
    "        return transformed\n",
    "    \n",
    "    tf_ii_dict_sorted = chunk_dict(transformed, chunks)\n",
    "    \n",
    "    def split_dict(tf_ii_dict_sorted, chunks):      \n",
    "        i = itertools.cycle(range(chunks))       \n",
    "        split = [dict() for _ in range(chunks)]\n",
    "        for k, v in tf_ii_dict_sorted.items():\n",
    "            split[next(i)][k] = v\n",
    "        return split\n",
    "    \n",
    "    #for doc in tf_ii_dict_sorted:\n",
    "    #    tf_ii_dict_sorted[doc] = split_dict(tf_ii_dict_sorted[doc],chunks)\n",
    "        \n",
    "    print(\"\\nChunked inverted index:\")\n",
    "    print(tf_ii_dict_sorted[\"diary\"])\n",
    "    \n",
    "    def sort_chunks(tf_ii_dict_sorted):\n",
    "        for term, tier in tf_ii_dict_sorted.items():\n",
    "            for tier, lst in tf_ii_dict_sorted[term].items():\n",
    "                lst.sort()\n",
    "        return tf_ii_dict_sorted\n",
    "    \n",
    "    tf_ii_dict_sorted = sort_chunks(tf_ii_dict_sorted)\n",
    "    \n",
    "    print(\"\\nChunked inverted index with sorted chunks (tiered index):\")\n",
    "    print(tf_ii_dict_sorted[\"diary\"])\n",
    "    return tf_ii_dict_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function is tested on term 'human'. It performs following steps:\n",
      "\n",
      "Inverted index:\n",
      "{66: 0.00558659217877095, 67: 0.00510204081632653, 141: 0.006097560975609756, 246: 0.006802721088435374, 885: 0.018292682926829267, 1179: 0.00625, 1444: 0.018633540372670808, 1570: 0.006622516556291391, 1806: 0.008, 1807: 0.004975124378109453, 1830: 0.0064516129032258064, 1933: 0.004524886877828055, 2114: 0.006211180124223602, 2176: 0.009900990099009901, 2177: 0.00625, 2679: 0.006134969325153374, 2723: 0.012269938650306749, 3085: 0.007194244604316547}\n",
      "\n",
      "Sorted inverted index by tf(term, doc):\n",
      "{1444: 0.018633540372670808, 885: 0.018292682926829267, 2723: 0.012269938650306749, 2176: 0.009900990099009901, 1806: 0.008, 3085: 0.007194244604316547, 246: 0.006802721088435374, 1570: 0.006622516556291391, 1830: 0.0064516129032258064, 1179: 0.00625, 2177: 0.00625, 2114: 0.006211180124223602, 2679: 0.006134969325153374, 141: 0.006097560975609756, 66: 0.00558659217877095, 67: 0.00510204081632653, 1807: 0.004975124378109453, 1933: 0.004524886877828055}\n",
      "\n",
      "Sorted inverted index without tf(term,doc) values:\n",
      "[1444, 885, 2723, 2176, 1806, 3085, 246, 1570, 1830, 1179, 2177, 2114, 2679, 141, 66, 67, 1807, 1933]\n",
      "\n",
      "Chunked inverted index:\n",
      "{0: [1444, 885, 2723, 2176, 1806], 1: [3085, 246, 1570, 1830, 1179], 2: [2177, 2114, 2679, 141], 3: [66, 67, 1807, 1933]}\n",
      "\n",
      "Chunked inverted index with sorted chunks (tiered index):\n",
      "{0: [885, 1444, 1806, 2176, 2723], 1: [246, 1179, 1570, 1830, 3085], 2: [141, 2114, 2177, 2679], 3: [66, 67, 1807, 1933]}\n"
     ]
    }
   ],
   "source": [
    "# Call the function on the corpus\n",
    "tiered_index_dict = tiered_index(corpus, 4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intersection algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inter_one_list(p1,p2): #posting 1 list, posting 2 list\n",
    "    i=0\n",
    "    j=0\n",
    "    intersection = []\n",
    "    \n",
    "    while i < len(p1) and j < len(p2):\n",
    "        if p1[i] == p2[j]:\n",
    "            if i== 0 or p1[i] != p1[i-1]:\n",
    "                intersection.append(p1[i])\n",
    "            i += 1\n",
    "            j += 1           \n",
    "        elif p1[i] < p2[j]:\n",
    "            i += 1\n",
    "        else: # p[i] > p[j]\n",
    "            j += 1     \n",
    "    return intersection\n",
    "        \n",
    "def inter_n_lists(lst):\n",
    "    \n",
    "    rank_lst = sorted(lst, key = len)   \n",
    "    intersection = []\n",
    "    for i in range(len(rank_list)):\n",
    "        if len(rank_lst) <= 1:\n",
    "            intersection = rank_lst[0]\n",
    "        elif len(rank_lst) == 2:\n",
    "            intersection = inter_one_list(rank_lst[0], rank_lst[1])\n",
    "        else:\n",
    "            if i == 0:\n",
    "                intersection = inter_one_list(rank_lst[i], rank_lst[i+1])\n",
    "            elif i > 1:\n",
    "                intersection = inter_one_list(rank_lst[i], intersection)\n",
    "    return intersection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve postings for terms in query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PLAIN-1</td>\n",
       "      <td>why deep fried foods may cause cancer in the l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PLAIN-1007</td>\n",
       "      <td>ddt - - persistent organic pollutants , indust...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PLAIN-101</td>\n",
       "      <td>how to treat multiple sclerosis with diet mult...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PLAIN-1017</td>\n",
       "      <td>detoxification - - cancer , raw food , heart h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PLAIN-1027</td>\n",
       "      <td>dietary guidelines - - heart disease , cardiov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PLAIN-1038</td>\n",
       "      <td>dogs - - meat , animal products , cats , heart...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>PLAIN-1049</td>\n",
       "      <td>dr. david spence - - heart health , heart dise...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PLAIN-1065</td>\n",
       "      <td>dr. walter kempner - - mortality , heart disea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PLAIN-1077</td>\n",
       "      <td>dulse - - thyroid health , hijiki , sushi , io...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PLAIN-1087</td>\n",
       "      <td>easter island - - mortality , muscle strength ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID                                               TEXT\n",
       "0     PLAIN-1  why deep fried foods may cause cancer in the l...\n",
       "1  PLAIN-1007  ddt - - persistent organic pollutants , indust...\n",
       "2   PLAIN-101  how to treat multiple sclerosis with diet mult...\n",
       "3  PLAIN-1017  detoxification - - cancer , raw food , heart h...\n",
       "4  PLAIN-1027  dietary guidelines - - heart disease , cardiov...\n",
       "5  PLAIN-1038  dogs - - meat , animal products , cats , heart...\n",
       "6  PLAIN-1049  dr. david spence - - heart health , heart dise...\n",
       "7  PLAIN-1065  dr. walter kempner - - mortality , heart disea...\n",
       "8  PLAIN-1077  dulse - - thyroid health , hijiki , sushi , io...\n",
       "9  PLAIN-1087  easter island - - mortality , muscle strength ..."
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loead some queries for testing\n",
    "queries_text = pd.read_csv('nfcorpus/dev.all.queries', sep='\\t', names=['ID', 'TEXT'])\n",
    "queries_text.head(10)\n",
    "#queries_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_postings(query):   \n",
    "    tokens = query.split()\n",
    "    postings_tiers = []\n",
    "    \n",
    "    for i in range(0,len(tokens)): \n",
    "        try:\n",
    "            dic = {}\n",
    "            dic[tokens[i]] = tiered_index_dict[tokens[i]]\n",
    "            postings_tiers.append(dic)           \n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    return postings_tiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating through tieres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1845\n"
     ]
    }
   ],
   "source": [
    "# Retrieve postings for query tokens from tiered index\n",
    "query_postigs = retrieve_postings(queries_text.iloc[0][1])\n",
    "\n",
    "# Extract i-th tier from all postings\n",
    "\n",
    "def tieres(query_postigs, n):\n",
    "    l = []\n",
    "    for i in range(len(query_postigs)): # for each posting\n",
    "        d = query_postigs[i]\n",
    "        key = [key for key in d.keys()][0]\n",
    "        l.append(d[key][n]) # append i-th tier list\n",
    "\n",
    "    return l\n",
    "\n",
    "l = tieres(query_postigs,3)\n",
    " # change to for in in no of tieres\n",
    "print(len(l))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Intersection on tieres \n",
    "for i in range(325):\n",
    "    retrieve_postings(queries_text.iloc[0][1])\n",
    "    #print(inter_n_lists(tieres(retrieve_postings(queries_text.iloc[i][1]),2)))\n",
    "\n",
    "    print(queries_text.iloc[i][1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[538, 569, 571, 572, 1772, 2118, 2607, 2825, 2832, 2925, 2948, 2974, 2976]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(inter_n_lists(tieres(retrieve_postings('heart disease'),0)))\n",
    "\n",
    "a = retrieve_postings('sugar blod')\n",
    "a = tieres(a,0)\n",
    "a = inter_n_lists(a)\n",
    "\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
