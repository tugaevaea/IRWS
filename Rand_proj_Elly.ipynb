{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import faiss                   # make faiss available\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy.sparse as sp\n",
    "import collections\n",
    "import math\n",
    "import copy\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic 4: Efficient Vector Space Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data and vectorize using TfidfVectorizer\n",
    "https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n",
    "\n",
    "TF-IDF weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MED-118</td>\n",
       "      <td>alkylphenols human milk relations dietary habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MED-329</td>\n",
       "      <td>phosphate vascular toxin pubmed ncbi abstract ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MED-330</td>\n",
       "      <td>dietary phosphorus acutely impairs endothelial...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MED-332</td>\n",
       "      <td>public health impact dietary phosphorus excess...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MED-334</td>\n",
       "      <td>differences total vitro digestible phosphorus ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3188</th>\n",
       "      <td>MED-5367</td>\n",
       "      <td>relationship plasma carotenoids depressive sym...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3189</th>\n",
       "      <td>MED-5368</td>\n",
       "      <td>suicide mortality relation dietary intake num ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3190</th>\n",
       "      <td>MED-5369</td>\n",
       "      <td>suicide mortality european union pubmed ncbi a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3191</th>\n",
       "      <td>MED-5370</td>\n",
       "      <td>long chain omega num fatty acids intake fish c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3192</th>\n",
       "      <td>MED-5371</td>\n",
       "      <td>omega num fatty acids treatment depression sys...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3193 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID                                               TEXT\n",
       "0      MED-118  alkylphenols human milk relations dietary habi...\n",
       "1      MED-329  phosphate vascular toxin pubmed ncbi abstract ...\n",
       "2      MED-330  dietary phosphorus acutely impairs endothelial...\n",
       "3      MED-332  public health impact dietary phosphorus excess...\n",
       "4      MED-334  differences total vitro digestible phosphorus ...\n",
       "...        ...                                                ...\n",
       "3188  MED-5367  relationship plasma carotenoids depressive sym...\n",
       "3189  MED-5368  suicide mortality relation dietary intake num ...\n",
       "3190  MED-5369  suicide mortality european union pubmed ncbi a...\n",
       "3191  MED-5370  long chain omega num fatty acids intake fish c...\n",
       "3192  MED-5371  omega num fatty acids treatment depression sys...\n",
       "\n",
       "[3193 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = pd.read_csv('nfcorpus/dev.docs', sep='\\t', names=['ID', 'TEXT'])\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1140</th>\n",
       "      <td>MED-2421</td>\n",
       "      <td>birth weight head circumference prenatal expos...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID                                               TEXT\n",
       "1140  MED-2421  birth weight head circumference prenatal expos..."
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find the id of med-2421\n",
    "med_id = corpus['ID'] == \"MED-2421\"\n",
    "med_id = corpus[med_id]\n",
    "med_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create token list out of document\n",
    "def tokenize(string):\n",
    "    return string.split()\n",
    "\n",
    "# apply term frequencies for each a single string (document)\n",
    "def tf(string): \n",
    "    # create bag of words from the string\n",
    "    bow = tokenize(string)\n",
    "    \n",
    "    tf_dict = {}\n",
    "    for word in bow:\n",
    "        if word in tf_dict:\n",
    "            tf_dict[word] += 1\n",
    "        else:\n",
    "            tf_dict[word] = 1\n",
    "            \n",
    "    for word in tf_dict:\n",
    "        tf_dict[word] = tf_dict[word] / len(bow)\n",
    "    \n",
    "    return tf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008547008547008548"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# We then call our function on every doc and store all these tf dictionaries. \n",
    "tf_dict = {}\n",
    "for index, row in corpus.iterrows():\n",
    "    doc_dict = tf(row['TEXT'])\n",
    "    tf_dict[index] = doc_dict\n",
    "\n",
    "# test if tfDict was created correctly\n",
    "tf_dict[0][\"alkylphenols\"]\n",
    "# alkylphenols for doc 0 : 0.008547008547008548"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3193\n"
     ]
    }
   ],
   "source": [
    "# total number of documents in corpus\n",
    "no_of_docs = len(corpus.index)\n",
    "print(no_of_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# term - key, number of docs term occured in\n",
    "def count_occurances():\n",
    "    count_dict = {}\n",
    "    for key in tf_dict:\n",
    "        for key in tf_dict[key]:\n",
    "            if key in count_dict:\n",
    "                count_dict[key] += 1\n",
    "            else:\n",
    "                count_dict[key] = 1\n",
    "    return count_dict\n",
    "\n",
    "# test if count_occurances works\n",
    "count_oc = count_occurances()\n",
    "count_oc[\"alkylphenols\"] # checked with Elina, good\n",
    "\n",
    "# number of alkylphenols occurence in entire corpus = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.122806043659469"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# having total number of documents and number of occurances of each word in entire corpus we can calculate \n",
    "# idf for each term as log(total # of documents / # of documents with term in it)\n",
    "\n",
    "# idf is calculated per each term, thus we create dictionary with term as a key and idf as a value\n",
    "def idf():\n",
    "    \n",
    "    idf_dict = {}\n",
    "    for key in count_oc:\n",
    "        idf_dict[key] = math.log(no_of_docs/count_oc[key])\n",
    "    return idf_dict\n",
    "\n",
    "idf = idf()\n",
    "\n",
    "# test if idf function works\n",
    "idf[\"alkylphenols\"]\n",
    "\n",
    "# alkylphenols idf = 6.122806043659469"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result from def:\n",
      "0.05233167558683307\n",
      "Manual result:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05233167558683307"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cosntructing the final tf-idf dictionary; tf-idf is calculated as tf-idf(t, d) = tf(t, d) * idf(t)\n",
    "# so for each key in tf dict we have to miltiply it with corresponsinf idf value\n",
    "\n",
    "def tf_idf():\n",
    "    d = copy.deepcopy(tf_dict)\n",
    "    for doc, value in d.items():\n",
    "        for word, value in d[doc].items():\n",
    "            d[doc][word] = value * idf[word]\n",
    "    return d\n",
    "\n",
    "# test if tf_idf works\n",
    "a = tf_idf()\n",
    "print('Result from def:')\n",
    "print(a[0][\"alkylphenols\"])\n",
    "\n",
    "# excpected result for (term, doc) --> (alkylphenols, 0) =  0.008547008547008548 * 6.122806043659469 = 0.05\n",
    "print('Manual result:')\n",
    "idf[\"alkylphenols\"] * tf_dict[0][\"alkylphenols\"]\n",
    "\n",
    "# it works :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alkylphenols</th>\n",
       "      <th>human</th>\n",
       "      <th>milk</th>\n",
       "      <th>relations</th>\n",
       "      <th>dietary</th>\n",
       "      <th>habits</th>\n",
       "      <th>central</th>\n",
       "      <th>taiwan</th>\n",
       "      <th>pubmed</th>\n",
       "      <th>ncbi</th>\n",
       "      <th>...</th>\n",
       "      <th>tuscany</th>\n",
       "      <th>studies-depression</th>\n",
       "      <th>suicides</th>\n",
       "      <th>eurosave</th>\n",
       "      <th>self-inflicted</th>\n",
       "      <th>eurostat</th>\n",
       "      <th>upward</th>\n",
       "      <th>suicide-recording</th>\n",
       "      <th>scarcity</th>\n",
       "      <th>trim-and-fill</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.052332</td>\n",
       "      <td>0.041372</td>\n",
       "      <td>0.079999</td>\n",
       "      <td>0.046407</td>\n",
       "      <td>0.021178</td>\n",
       "      <td>0.060818</td>\n",
       "      <td>0.029952</td>\n",
       "      <td>0.047041</td>\n",
       "      <td>0.002278</td>\n",
       "      <td>0.002334</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001777</td>\n",
       "      <td>0.001820</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028372</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022663</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001625</td>\n",
       "      <td>0.001665</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001549</td>\n",
       "      <td>0.001588</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26951 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   alkylphenols     human      milk  relations   dietary    habits   central  \\\n",
       "0      0.052332  0.041372  0.079999   0.046407  0.021178  0.060818  0.029952   \n",
       "1      0.000000  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "2      0.000000  0.000000  0.000000   0.000000  0.028372  0.000000  0.000000   \n",
       "3      0.000000  0.000000  0.000000   0.000000  0.022663  0.000000  0.000000   \n",
       "4      0.000000  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "     taiwan    pubmed      ncbi  ...  tuscany  studies-depression  suicides  \\\n",
       "0  0.047041  0.002278  0.002334  ...      0.0                 0.0       0.0   \n",
       "1  0.000000  0.001777  0.001820  ...      0.0                 0.0       0.0   \n",
       "2  0.000000  0.000000  0.000000  ...      0.0                 0.0       0.0   \n",
       "3  0.000000  0.001625  0.001665  ...      0.0                 0.0       0.0   \n",
       "4  0.000000  0.001549  0.001588  ...      0.0                 0.0       0.0   \n",
       "\n",
       "   eurosave  self-inflicted  eurostat  upward  suicide-recording  scarcity  \\\n",
       "0       0.0             0.0       0.0     0.0                0.0       0.0   \n",
       "1       0.0             0.0       0.0     0.0                0.0       0.0   \n",
       "2       0.0             0.0       0.0     0.0                0.0       0.0   \n",
       "3       0.0             0.0       0.0     0.0                0.0       0.0   \n",
       "4       0.0             0.0       0.0     0.0                0.0       0.0   \n",
       "\n",
       "   trim-and-fill  \n",
       "0            0.0  \n",
       "1            0.0  \n",
       "2            0.0  \n",
       "3            0.0  \n",
       "4            0.0  \n",
       "\n",
       "[5 rows x 26951 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# First we have to build TF-IDF matrix based on obtain dictionary. \n",
    "# Rows will correspond to docs in the corpus, while columns will represent unique words\n",
    "\n",
    "#              word1       ...          wordn\n",
    "#  doc1   tf_idf_value   ...      tf_idf_value\n",
    "#  ...    tf_idf_value   ...      tf_idf_value\n",
    "#  docn   tf_idf_value   ...      tf_idf_value\n",
    "#\n",
    "\n",
    "tf_idf_matrix = pd.DataFrame.from_dict(a, orient = 'index').fillna(0) # if word does not appear in doc we change NaN to 0\n",
    "tf_idf_matrix = tf_idf_matrix.sort_index()\n",
    "tf_idf_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alkylphenols         0.052332\n",
      "human                0.041372\n",
      "milk                 0.079999\n",
      "relations            0.046407\n",
      "dietary              0.021178\n",
      "                       ...   \n",
      "eurostat             0.000000\n",
      "upward               0.000000\n",
      "suicide-recording    0.000000\n",
      "scarcity             0.000000\n",
      "trim-and-fill        0.000000\n",
      "Name: 0, Length: 26951, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9999999999999996"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we have to compare docs by computing cosine similarity between each vector (row) in dataframe\n",
    "# For that we need to obtain 1. vector magnitude 2. dot product between two vectors\n",
    "\n",
    "def vector_magnitude(v):\n",
    "    return np.linalg.norm(v)\n",
    "\n",
    "def dot_product(v1, v2):\n",
    "    return np.dot(v1,v2)\n",
    "\n",
    "# Creating cosine similarity table (should be 3193 x 3193)\n",
    "def cosine_similarity(v1, v2):\n",
    "    return dot_product(v1, v2)/ (vector_magnitude(v1) * vector_magnitude(v2))\n",
    "print(tf_idf_matrix.iloc[0])\n",
    "cosine_similarity(tf_idf_matrix.iloc[0],tf_idf_matrix.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Projections\n",
    "**Hashing algorithm:**\n",
    "<br>1.Choose a set of *$M$* random vectors ${r_1, r_2, ..., r_M}$ in the original high-dimensional vectors space (vector length $|V|$)\n",
    "<br>2.For each document TF-IDF vector d do:\n",
    " - Compute the inner (dot) product of doc and each random vector $r:Î¸(r, d) = \\sum_{i}^{|ð‘‰|}ð‘Ÿ_ð‘–âˆ—ð‘‘_ð‘–$\n",
    " - Hash each inner product: $h(d, r_k) = 1$ if $Î¸(r, d) > t$ (treshold), else 0\n",
    "\n",
    "3.Compute a new vector of hashes:\n",
    " - $dâ€™ = [h(d, r_1), h(d, r_2), ..., h(d, r_M)]$\n",
    " - The number of selected random vectors, *$M$*, is the dimensionality of hashed vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a function for  creating a set of M random vectors with the dimension dim\n",
    "def get_random_vectors(dim,m):\n",
    "    return np.random.random((m, dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimension of the set of random vectors:  (1000, 26951)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.06284314, 0.51746825, 0.9356375 , ..., 0.81412037, 0.24848416,\n",
       "       0.09751716])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test the get_random_vectors\n",
    "vocab_size = len(tf_idf_matrix.columns)\n",
    "np.random.seed(0)\n",
    "m = 1000\n",
    "random_vectors = get_random_vectors(vocab_size, m)\n",
    "print('dimension of the set of random vectors: ', random_vectors.shape)\n",
    "random_vectors[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d - document vector\n",
    "# rnd_vec - set of random vectors\n",
    "# t - threshold\n",
    "def norm(vec):\n",
    "    return vec/np.linalg.norm(vec)\n",
    "    \n",
    "    \n",
    "def compute_hash(docs, rnd_vec, t):\n",
    "    hashed_doc_vectors = []\n",
    "    #for each document in document collection\n",
    "    for doc in docs:\n",
    "        hashed_dot_product = []\n",
    "        inner_product = doc.dot(rnd_vec.transpose())\n",
    "        for i in inner_product:  \n",
    "            if i>t:\n",
    "                hashed_dot_product.append(1)\n",
    "            else:\n",
    "                hashed_dot_product.append(0)\n",
    "        hashed_doc_vectors.append(hashed_dot_product)\n",
    "    return np.array(hashed_doc_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4477014678356913\n",
      "94.52892786806335\n"
     ]
    }
   ],
   "source": [
    "print(np.linalg.norm(doc_vectors[0]))\n",
    "print(np.linalg.norm(random_vectors[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.11688967 0.09241034 0.178688   ... 0.         0.         0.        ]\n",
      "0.9999999999999999\n",
      "[0.00580577 0.00756583 0.0063765  ... 0.00439207 0.00476961 0.00070877]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#normalize doc and random vectors\n",
    "doc_vectors = tf_idf_matrix.values\n",
    "norm_doc_vectors = []\n",
    "for doc in doc_vectors:\n",
    "    norm_doc_vectors.append(norm(doc))\n",
    "norm_doc_vectors = np.array(norm_doc_vectors)\n",
    "norm_rand_vectors = []\n",
    "for rand in random_vectors:\n",
    "    norm_rand_vectors.append(norm(rand))\n",
    "norm_rand_vectors = np.array(norm_rand_vectors)\n",
    "print(norm_doc_vectors[0])\n",
    "print(np.linalg.norm(norm_doc_vectors[0]))\n",
    "print(norm_rand_vectors[0])\n",
    "print(np.linalg.norm(norm_rand_vectors[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03694885 0.03347691 0.03509818 0.03908683 0.03716179 0.03458965\n",
      " 0.03233641 0.0318238  0.03385612 0.03688681 0.03829495 0.03366506\n",
      " 0.03345653 0.0405509  0.03487201 0.0398219  0.0369301  0.03136072\n",
      " 0.03701938 0.03681286 0.03807685 0.03580785 0.03408752 0.0365292\n",
      " 0.03662782 0.03549769 0.04245463 0.03677506 0.03613482 0.04192047\n",
      " 0.0339268  0.03569237 0.03441085 0.0394335  0.02881328 0.03814459\n",
      " 0.03799954 0.03364659 0.03205626 0.03212065 0.03256582 0.04031167\n",
      " 0.03182015 0.03658475 0.03345521 0.03114847 0.03237542 0.03913713\n",
      " 0.03877613 0.03799149 0.03301124 0.03434014 0.03582192 0.03694325\n",
      " 0.03342712 0.03569465 0.03696214 0.03057352 0.03821359 0.03847459\n",
      " 0.0323195  0.03541804 0.03646257 0.03153249 0.03507998 0.03491626\n",
      " 0.03095135 0.0397747  0.03678468 0.03572512 0.03517627 0.03644772\n",
      " 0.03632206 0.03460287 0.03848196 0.0331872  0.04109994 0.03824099\n",
      " 0.03728287 0.03753215 0.03939116 0.03947001 0.0380501  0.02880263\n",
      " 0.02990037 0.03736004 0.03731446 0.03771752 0.03996156 0.03367961\n",
      " 0.03562893 0.0321773  0.03868956 0.03090648 0.03572334 0.03444277\n",
      " 0.035678   0.03782594 0.03731171 0.03530662 0.02987733 0.03350672\n",
      " 0.03374592 0.04055997 0.0368786  0.03481377 0.03183325 0.04176775\n",
      " 0.03845131 0.04103462 0.03836746 0.03958255 0.03059086 0.02929202\n",
      " 0.03817959 0.03762884 0.03734862 0.03840544 0.03560676 0.03521287\n",
      " 0.03782656 0.03398915 0.03310261 0.03445956 0.03403956 0.04119178\n",
      " 0.04022032 0.03931542 0.03099557 0.03641607 0.03770646 0.03567732\n",
      " 0.0392102  0.03020177 0.03583675 0.04046873 0.03594044 0.03554143\n",
      " 0.03187481 0.03595432 0.03685197 0.03509094 0.03926596 0.03218969\n",
      " 0.03428559 0.03540028 0.03732132 0.03389797 0.03851038 0.04137493\n",
      " 0.03532083 0.03326975 0.03775517 0.0355449  0.03434459 0.03401647\n",
      " 0.03315004 0.03313102 0.03766366 0.03664972 0.03170331 0.04252184\n",
      " 0.03171083 0.03200756 0.03683268 0.04123818 0.03567801 0.03478108\n",
      " 0.03277333 0.03421407 0.03630916 0.03493417 0.03344203 0.03832207\n",
      " 0.03610125 0.03673844 0.03618306 0.03162097 0.03429963 0.03629355\n",
      " 0.03334701 0.03647365 0.03482201 0.03913507 0.0404882  0.0302854\n",
      " 0.03742439 0.03697522 0.03282259 0.03396996 0.03634776 0.03881992\n",
      " 0.0368852  0.03492696 0.04209418 0.03116577 0.03536465 0.03221602\n",
      " 0.02981084 0.03348418 0.03410182 0.03363661 0.03396148 0.03950537\n",
      " 0.03503802 0.03462487 0.03721121 0.03897781 0.03363329 0.03802636\n",
      " 0.0396799  0.031925   0.03797943 0.0354902  0.03572103 0.03509512\n",
      " 0.03714127 0.04127178 0.03843892 0.03714847 0.0391503  0.03705552\n",
      " 0.04060778 0.03529173 0.03320848 0.03424138 0.03940956 0.03970959\n",
      " 0.03733991 0.02681655 0.03956934 0.03317472 0.03967375 0.03291553\n",
      " 0.03746543 0.03295866 0.03338784 0.03547005 0.04089997 0.034775\n",
      " 0.04382761 0.03847319 0.03695023 0.03309354 0.03486538 0.03499096\n",
      " 0.03654822 0.03695283 0.0376907  0.03451168 0.03530914 0.03024913\n",
      " 0.0363038  0.03903771 0.03372814 0.03718845 0.03977824 0.03666637\n",
      " 0.03430334 0.03098697 0.04221464 0.03602425 0.03823997 0.03724302\n",
      " 0.04028918 0.03702256 0.03641698 0.0358854  0.03023124 0.03355408\n",
      " 0.03341083 0.03498372 0.03959911 0.03627053 0.03998076 0.03456952\n",
      " 0.03902983 0.04081212 0.03248372 0.03374391 0.03513261 0.03811019\n",
      " 0.03526199 0.03789988 0.0390556  0.03897306 0.03296269 0.04201912\n",
      " 0.04317558 0.0332386  0.03499609 0.03160487 0.04032791 0.04028795\n",
      " 0.03297871 0.03501525 0.03704373 0.04336092 0.03699606 0.03922375\n",
      " 0.03808656 0.03757203 0.03517601 0.03689215 0.04609166 0.03883453\n",
      " 0.03853175 0.03667817 0.03215416 0.03595845 0.03536458 0.03426885\n",
      " 0.03401617 0.03954352 0.03201699 0.03750699 0.03510169 0.042982\n",
      " 0.03333396 0.0329824  0.03264695 0.03717253 0.02931181 0.03358573\n",
      " 0.04166563 0.03205554 0.03437509 0.03573368 0.03050705 0.03505165\n",
      " 0.03634477 0.03702141 0.03727464 0.03913096 0.04332953 0.03547616\n",
      " 0.03362143 0.03562682 0.03271008 0.03216593 0.03637654 0.03114499\n",
      " 0.03302807 0.03650076 0.03998715 0.03772677 0.03141592 0.03677772\n",
      " 0.0330103  0.03884464 0.03465192 0.03995779 0.0351476  0.03441407\n",
      " 0.04004631 0.02948338 0.03526407 0.03883523 0.03445478 0.03721997\n",
      " 0.03509326 0.03856859 0.03209161 0.03488723 0.03582393 0.03860628\n",
      " 0.0363011  0.02852994 0.03462968 0.03966323 0.03167785 0.03741173\n",
      " 0.03503942 0.04068465 0.04026104 0.03363562 0.03532295 0.04051687\n",
      " 0.03850996 0.04027811 0.03974075 0.03593209 0.03739241 0.03763274\n",
      " 0.03878155 0.03920715 0.03011027 0.03132173 0.03731705 0.04317951\n",
      " 0.03700714 0.03570398 0.03495962 0.03891809 0.03865103 0.03561189\n",
      " 0.03172001 0.03742452 0.03456876 0.03227774 0.03289949 0.04185071\n",
      " 0.03466621 0.04076756 0.03867922 0.03952612 0.04105327 0.03701846\n",
      " 0.03638768 0.03542384 0.03431277 0.04097823 0.03596513 0.03686549\n",
      " 0.03595858 0.03832814 0.03599701 0.02843449 0.0296074  0.04273049\n",
      " 0.03643821 0.03984238 0.0333563  0.03721564 0.03865259 0.03274608\n",
      " 0.03780667 0.04036897 0.03353955 0.03588574 0.0385289  0.03299151\n",
      " 0.03733959 0.0388739  0.03909658 0.03781571 0.0434542  0.03784007\n",
      " 0.03857967 0.03363467 0.03825311 0.03771587 0.03392186 0.03478632\n",
      " 0.03446976 0.03415546 0.03282061 0.03713827 0.03694952 0.03601439\n",
      " 0.02980763 0.03263999 0.03603002 0.03838404 0.03348047 0.02983793\n",
      " 0.03406417 0.03580519 0.03956161 0.03435186 0.03796798 0.03786557\n",
      " 0.03850766 0.02629035 0.03408642 0.03457152 0.03238688 0.03291561\n",
      " 0.03638224 0.03426868 0.03957443 0.0355202  0.04041268 0.03821504\n",
      " 0.03478336 0.03305272 0.03557261 0.03914215 0.03128884 0.03847244\n",
      " 0.03604787 0.04333399 0.03143123 0.03591669 0.02991177 0.03520139\n",
      " 0.03396279 0.03478387 0.03397635 0.03648372 0.04028851 0.03687436\n",
      " 0.03591627 0.03278679 0.03836328 0.03819129 0.03231362 0.03689659\n",
      " 0.03688164 0.03784715 0.0327967  0.03665464 0.03824152 0.03580517\n",
      " 0.04059598 0.03717944 0.03546881 0.0338499  0.03722181 0.03622234\n",
      " 0.03969858 0.03538211 0.03763119 0.03375632 0.03913803 0.03486156\n",
      " 0.03220628 0.03682614 0.03751986 0.03293616 0.03329642 0.03733975\n",
      " 0.03815451 0.03078446 0.03578272 0.03825391 0.03629118 0.03599741\n",
      " 0.03579165 0.03925263 0.03494886 0.0384049  0.03353259 0.03195081\n",
      " 0.0311652  0.03424157 0.03695584 0.03866877 0.03205878 0.03576847\n",
      " 0.0346075  0.03571616 0.03329449 0.03322826 0.03957162 0.03139291\n",
      " 0.03723861 0.03669514 0.03398766 0.03561299 0.03829511 0.03186156\n",
      " 0.03066032 0.03201071 0.03485571 0.03506695 0.03664941 0.03233417\n",
      " 0.03536487 0.03721768 0.03725256 0.03305089 0.03630634 0.03418797\n",
      " 0.0345786  0.04008754 0.03345936 0.03236916 0.03298901 0.03178711\n",
      " 0.03604099 0.03946886 0.03262337 0.03387097 0.03378524 0.03506869\n",
      " 0.03668919 0.03638946 0.03119657 0.03661704 0.03582869 0.03236123\n",
      " 0.03231448 0.02912388 0.03669571 0.03502397 0.04157444 0.03880246\n",
      " 0.03943896 0.04085256 0.03775594 0.03701408 0.03261823 0.03306938\n",
      " 0.03222772 0.03970793 0.03322607 0.03731408 0.0386101  0.03891761\n",
      " 0.03341272 0.03260625 0.03340815 0.039585   0.03724698 0.03846856\n",
      " 0.03475431 0.03537129 0.03826599 0.04107671 0.03609384 0.03932146\n",
      " 0.0318024  0.03913529 0.03622737 0.03137975 0.03308828 0.03006556\n",
      " 0.03736782 0.03129404 0.04158591 0.03902771 0.03137055 0.03441651\n",
      " 0.0353693  0.03498213 0.03653914 0.04377078 0.03387898 0.03827459\n",
      " 0.03611498 0.03988222 0.03674091 0.03701654 0.03469119 0.0326777\n",
      " 0.03988022 0.03568768 0.03567673 0.03582894 0.03822724 0.03849702\n",
      " 0.04106368 0.03507204 0.02999433 0.02993451 0.03058096 0.0409387\n",
      " 0.03802174 0.0345077  0.03621662 0.03390315 0.03556631 0.03125177\n",
      " 0.04144201 0.0423101  0.03736091 0.03398292 0.03839274 0.03139999\n",
      " 0.03848692 0.04066347 0.03193572 0.03656165 0.03903864 0.03147422\n",
      " 0.0384973  0.03350182 0.03756311 0.03499837 0.03925022 0.0321162\n",
      " 0.03271929 0.03790644 0.03337047 0.03515978 0.03353188 0.03140466\n",
      " 0.03847552 0.03710921 0.03581098 0.0348327  0.03917487 0.0373099\n",
      " 0.03989495 0.03600017 0.03852012 0.03943943 0.03601542 0.03545719\n",
      " 0.03368927 0.03797308 0.02429874 0.03861529 0.04017858 0.03907097\n",
      " 0.03943951 0.03968406 0.03257422 0.04152906 0.03832829 0.0337699\n",
      " 0.0353572  0.03033853 0.03524248 0.0355568  0.03543583 0.03568217\n",
      " 0.0390594  0.03179463 0.03820189 0.03046971 0.03758454 0.03559481\n",
      " 0.03369913 0.03451153 0.03408771 0.03399262 0.03604792 0.03701243\n",
      " 0.03457561 0.03968499 0.03261393 0.03893713 0.03853207 0.03910343\n",
      " 0.03321068 0.03955241 0.0363094  0.03051099 0.0341798  0.03962883\n",
      " 0.03830489 0.03589611 0.03602198 0.04215912 0.03775385 0.03487055\n",
      " 0.03548717 0.03715923 0.03224184 0.03718472 0.03700967 0.03390242\n",
      " 0.03525278 0.0369605  0.03462017 0.03760657 0.03637939 0.03625984\n",
      " 0.04300475 0.03889973 0.03071735 0.03040942 0.03584935 0.03692214\n",
      " 0.03932495 0.03470963 0.03166122 0.03150898 0.03750334 0.03993149\n",
      " 0.03328964 0.03567016 0.03805844 0.03846857 0.039404   0.03361\n",
      " 0.03523188 0.03541945 0.03622215 0.03531864 0.03380919 0.03895167\n",
      " 0.03690388 0.03865153 0.03567346 0.0382243  0.03223751 0.03996791\n",
      " 0.04317689 0.03833891 0.0253993  0.03141268 0.03932693 0.0407882\n",
      " 0.03448341 0.03418943 0.03912377 0.03758701 0.03474181 0.03632056\n",
      " 0.03790074 0.03428774 0.03321643 0.03319962 0.03162629 0.03602673\n",
      " 0.03541092 0.03237148 0.03395725 0.03604027 0.03686458 0.04233026\n",
      " 0.03519377 0.0299613  0.03161425 0.04121953 0.04090032 0.03707238\n",
      " 0.03300328 0.03449635 0.03495704 0.03598006 0.03214827 0.04107855\n",
      " 0.03788946 0.03705395 0.03760614 0.04184718 0.04149448 0.03937343\n",
      " 0.03527233 0.03459704 0.03123112 0.03149397 0.03620673 0.04157155\n",
      " 0.03684324 0.03337608 0.03836473 0.03642471 0.03222301 0.04096197\n",
      " 0.03442469 0.03784244 0.0345     0.03963425 0.03506005 0.03657562\n",
      " 0.03910814 0.04131455 0.03933551 0.03578698 0.03250652 0.03843236\n",
      " 0.03548736 0.03496118 0.03731103 0.03924485 0.04128953 0.03423412\n",
      " 0.0353594  0.0341225  0.03286082 0.03512101 0.04005421 0.03796818\n",
      " 0.03955897 0.02865788 0.03311725 0.02930092 0.0413586  0.03750238\n",
      " 0.03611179 0.03493158 0.03919185 0.03324321 0.03218737 0.04085622\n",
      " 0.03592448 0.03763341 0.04004464 0.02813958 0.03016708 0.03813276\n",
      " 0.03295692 0.03341595 0.03313312 0.040626   0.03953607 0.03886614\n",
      " 0.03599196 0.04048717 0.03467094 0.03152415 0.04055459 0.03725477\n",
      " 0.03352189 0.04220134 0.03553669 0.03682863 0.03549895 0.03777156\n",
      " 0.03870478 0.03445267 0.03461491 0.03552107 0.03670915 0.03953365\n",
      " 0.03940143 0.03725895 0.03866252 0.03823526 0.03270054 0.03304799\n",
      " 0.03737784 0.04063624 0.03564949 0.04082546 0.03301041 0.03287588\n",
      " 0.03513517 0.04092103 0.0329367  0.0296017  0.03520088 0.03221047\n",
      " 0.03336344 0.03520841 0.03742339 0.03403854 0.03442158 0.03584926\n",
      " 0.04014544 0.03867868 0.03684323 0.03534753 0.03676863 0.03105795\n",
      " 0.03782216 0.03635632 0.04129222 0.03480856 0.0350477  0.03822752\n",
      " 0.038932   0.04007648 0.03809291 0.03396538 0.03530222 0.03523169\n",
      " 0.03800426 0.03425501 0.03343487 0.03682195 0.03639814 0.03078897\n",
      " 0.035728   0.03653419 0.03268623 0.03778913 0.0359643  0.04181519\n",
      " 0.04071146 0.03509035 0.03941315 0.03310077 0.03736885 0.03862395\n",
      " 0.03525567 0.03771905 0.03338753 0.03457849 0.03921833 0.03037252\n",
      " 0.03563431 0.03660094 0.03476737 0.03687294 0.03257552 0.03600306\n",
      " 0.03633682 0.03558062 0.03056901 0.03964803 0.03295256 0.03423813\n",
      " 0.03460695 0.03722158 0.03728604 0.03257177 0.03258924 0.03408616\n",
      " 0.03133505 0.03844566 0.03799867 0.03188379 0.03434514 0.03859919\n",
      " 0.03313942 0.03824076 0.03495521 0.03910047 0.0346258  0.03390704\n",
      " 0.03991407 0.03219953 0.03419715 0.03560878]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "#### test compute_hash\n",
    "inn = norm_doc_vectors[120].dot(norm_rand_vectors.transpose())\n",
    "h = []\n",
    "for i in inn:\n",
    "    if i>0.03:\n",
    "        h.append(1)\n",
    "    else:\n",
    "        h.append(0)\n",
    "print(inn)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 36.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,\n",
       "       0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n",
       "       0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       0, 0, 1, 1, 1, 0, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "random_projections = compute_hash(norm_doc_vectors, norm_rand_vectors, 0.03)\n",
    "random_projections[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use random projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PLAIN-1</td>\n",
       "      <td>why deep fried foods may cause cancer in the l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PLAIN-1007</td>\n",
       "      <td>ddt - - persistent organic pollutants , indust...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PLAIN-101</td>\n",
       "      <td>how to treat multiple sclerosis with diet mult...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PLAIN-1017</td>\n",
       "      <td>detoxification - - cancer , raw food , heart h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PLAIN-1027</td>\n",
       "      <td>dietary guidelines - - heart disease , cardiov...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID                                               TEXT\n",
       "0     PLAIN-1  why deep fried foods may cause cancer in the l...\n",
       "1  PLAIN-1007  ddt - - persistent organic pollutants , indust...\n",
       "2   PLAIN-101  how to treat multiple sclerosis with diet mult...\n",
       "3  PLAIN-1017  detoxification - - cancer , raw food , heart h...\n",
       "4  PLAIN-1027  dietary guidelines - - heart disease , cardiov..."
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read queries \n",
    "queries_text = pd.read_csv('nfcorpus/dev.all.queries', sep='\\t', names=['ID', 'TEXT'])\n",
    "queries_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_query(string):\n",
    "    tokenized_query = string.split()\n",
    "    df_query = tf_idf_matrix[0:0] #dataframe of tf-idf weights of a query\n",
    "    df_query = df_query.append(pd.Series(0, index=df_query.columns), ignore_index=True)\n",
    "    for token in tokenized_query:\n",
    "        for col in df_query.columns:\n",
    "            if token == col:\n",
    "                df_query[col][0] = df_query[col][0] + 1 #raw term frequency\n",
    "    \n",
    "    df_query = df_query.replace(0, np.nan)\n",
    "    \n",
    "    df_query = np.log(df_query) + 1 #log term freq(as in the slides)\n",
    "    \n",
    "    df_query = df_query.fillna(0)\n",
    "    \n",
    "    for col in df_query.columns:\n",
    "        df_query[col][0] = df_query[col][0] * idf[col]\n",
    "        \n",
    "    return df_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = []\n",
    "for index, row in queries_text.iterrows():\n",
    "    queries.append(norm(vectorize_query(row['TEXT'])).values)\n",
    "queries = np.array(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = norm(vectorize_query(queries_text['TEXT'][0])).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.        0.0074371 0.        ... 0.        0.        0.       ]]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(queries[0])\n",
    "print(np.linalg.norm(queries[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use random projections on query\n",
    "query_projection = compute_hash(queries[0],norm_rand_vectors, 0.03)\n",
    "query_projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01418965 0.01312186 0.02024626 0.0151166  0.01711882 0.01474291\n",
      " 0.02001731 0.01987751 0.01506695 0.01631306 0.02229158 0.02160481\n",
      " 0.0197475  0.01924086 0.02121038 0.01692146 0.01361659 0.01874481\n",
      " 0.01875865 0.02152752 0.01949002 0.02404756 0.0190541  0.01526021\n",
      " 0.02133627 0.01762899 0.01601999 0.02082219 0.01608068 0.01799404\n",
      " 0.01764885 0.02008633 0.01997723 0.02103125 0.01126997 0.0222526\n",
      " 0.01824178 0.01600529 0.01816196 0.02189278 0.01901328 0.01861716\n",
      " 0.01905788 0.01938079 0.01745998 0.02195585 0.02136404 0.02282045\n",
      " 0.02137035 0.01927101 0.02318791 0.0268834  0.0218458  0.01664822\n",
      " 0.01925513 0.01584503 0.02334391 0.01853832 0.01943791 0.02389146\n",
      " 0.02256401 0.0198177  0.02492088 0.01892332 0.02234661 0.01925965\n",
      " 0.02373871 0.02394984 0.01434891 0.01819844 0.02037914 0.02501464\n",
      " 0.01632429 0.02132532 0.02264144 0.02847278 0.01673142 0.02757921\n",
      " 0.02040803 0.01772402 0.01790145 0.01929002 0.01821828 0.01692696\n",
      " 0.01796749 0.01744559 0.01413976 0.01675808 0.0172711  0.02639288\n",
      " 0.01994039 0.02017259 0.02519317 0.01575046 0.02136474 0.02105215\n",
      " 0.01867998 0.0139124  0.02103198 0.0197958  0.01889732 0.01372602\n",
      " 0.0183194  0.02247302 0.02296492 0.01628562 0.02175681 0.02265753\n",
      " 0.01859162 0.01946875 0.01904139 0.01503945 0.01618837 0.01826057\n",
      " 0.02262291 0.02114031 0.02132463 0.018232   0.01839866 0.01894103\n",
      " 0.01911281 0.01372814 0.02051021 0.01289777 0.01924181 0.01958055\n",
      " 0.01955627 0.01869321 0.01678965 0.01432766 0.01590378 0.0209028\n",
      " 0.02129855 0.01660206 0.01802289 0.01656753 0.01854589 0.01669307\n",
      " 0.02108782 0.01322493 0.02329648 0.01609496 0.01641632 0.01731001\n",
      " 0.02109452 0.01556046 0.02467731 0.01533578 0.01937706 0.02012608\n",
      " 0.02007105 0.01929343 0.02094884 0.02084887 0.01524727 0.02128944\n",
      " 0.01821275 0.02159913 0.01636759 0.01951726 0.01666228 0.02043642\n",
      " 0.01793553 0.02118618 0.02005042 0.0194204  0.02140429 0.02620843\n",
      " 0.01414677 0.01546655 0.0161197  0.02127736 0.01984422 0.0201772\n",
      " 0.02017563 0.02035818 0.02066411 0.01943955 0.01834846 0.01945034\n",
      " 0.02033174 0.02317409 0.01975647 0.01743377 0.02043212 0.02357914\n",
      " 0.01351928 0.01904916 0.02485452 0.02110658 0.02287308 0.02346456\n",
      " 0.02593968 0.01826717 0.01814638 0.01740922 0.01863736 0.02035712\n",
      " 0.02441133 0.02694204 0.01352829 0.01443507 0.01346806 0.01918746\n",
      " 0.01723394 0.01796215 0.02169895 0.014005   0.02238105 0.02003434\n",
      " 0.01911733 0.01791685 0.01639254 0.02783714 0.02459889 0.0209375\n",
      " 0.01857656 0.01613729 0.01567319 0.01508417 0.01901965 0.02161298\n",
      " 0.02324445 0.01732684 0.02347692 0.01568266 0.01854836 0.01576442\n",
      " 0.01716067 0.0247688  0.01822956 0.01937576 0.02192621 0.01894069\n",
      " 0.02568479 0.01713928 0.02506123 0.01556245 0.01639935 0.02173706\n",
      " 0.01643091 0.02332283 0.01945504 0.01614573 0.02032409 0.01570895\n",
      " 0.01836513 0.01814664 0.01797763 0.01421893 0.01551106 0.01605072\n",
      " 0.02229188 0.01678686 0.01330905 0.01630656 0.01938799 0.02299941\n",
      " 0.02103364 0.02186536 0.01628296 0.01074834 0.02115341 0.0228243\n",
      " 0.01923469 0.02169947 0.01904447 0.021208   0.02288207 0.01584115\n",
      " 0.01804332 0.0221103  0.01539704 0.0158505  0.02068223 0.01652766\n",
      " 0.02163886 0.01995253 0.01437131 0.01733933 0.0248449  0.01712261\n",
      " 0.01924761 0.02018092 0.01808027 0.02250542 0.02068571 0.0215376\n",
      " 0.0200285  0.01963983 0.02009257 0.02248557 0.01571198 0.01930589\n",
      " 0.0155099  0.01720022 0.02173397 0.0204296  0.02221335 0.02222587\n",
      " 0.02420652 0.02026039 0.02331428 0.02050368 0.01573841 0.01359544\n",
      " 0.02077601 0.02306522 0.01957112 0.0216556  0.02143508 0.01774773\n",
      " 0.02306957 0.02132537 0.01133048 0.01598049 0.0174692  0.02097311\n",
      " 0.01760594 0.02527973 0.01775388 0.01672631 0.01459347 0.02221021\n",
      " 0.0241534  0.02073572 0.02117548 0.01880044 0.01388545 0.01863933\n",
      " 0.02343499 0.01975089 0.01905432 0.01849551 0.01586166 0.0219736\n",
      " 0.02150546 0.01853974 0.02009715 0.01855778 0.01828358 0.01873797\n",
      " 0.0209165  0.01997339 0.02197194 0.02113346 0.01403193 0.0156046\n",
      " 0.02463758 0.01706195 0.01778004 0.02160675 0.01787418 0.01928402\n",
      " 0.02640897 0.01328011 0.02402671 0.02105096 0.02324286 0.02143684\n",
      " 0.01876923 0.01737852 0.01991004 0.01943709 0.01764714 0.02299947\n",
      " 0.02640927 0.02233025 0.01928071 0.01795663 0.01763533 0.02009517\n",
      " 0.02075838 0.0186682  0.02212252 0.02418568 0.01638766 0.01692837\n",
      " 0.01379025 0.01821203 0.01974422 0.01942055 0.02059626 0.02207983\n",
      " 0.01406232 0.02311378 0.01857968 0.01822605 0.02104448 0.01536397\n",
      " 0.02121822 0.0206824  0.02131112 0.01855952 0.01713182 0.01783451\n",
      " 0.01767867 0.01718129 0.0231763  0.01994346 0.01801033 0.0172299\n",
      " 0.02185231 0.01931304 0.02623219 0.0200023  0.02441905 0.01874517\n",
      " 0.02551604 0.01167779 0.01936656 0.01937858 0.01928655 0.02522006\n",
      " 0.01453877 0.01746304 0.01736606 0.02497054 0.01239944 0.02387783\n",
      " 0.01922745 0.01582161 0.02380783 0.02101282 0.01949572 0.01716018\n",
      " 0.02257855 0.02379931 0.01400002 0.01611397 0.02160383 0.01356908\n",
      " 0.01891482 0.01611503 0.01828319 0.01779333 0.02010704 0.0174204\n",
      " 0.02651531 0.02288283 0.02310162 0.02167084 0.0246662  0.02150458\n",
      " 0.02340147 0.02304206 0.01921217 0.0256394  0.02632461 0.01748592\n",
      " 0.02241822 0.01698033 0.02319801 0.0150998  0.01842672 0.02123385\n",
      " 0.020332   0.01534616 0.01750619 0.01834147 0.01755967 0.01930435\n",
      " 0.01626403 0.01736367 0.01903148 0.01803847 0.02052862 0.01808676\n",
      " 0.02112508 0.01946729 0.01530595 0.02157545 0.02367676 0.01761613\n",
      " 0.01925367 0.01812666 0.01981756 0.01918527 0.01814363 0.02116477\n",
      " 0.02158233 0.01721592 0.01872267 0.02249706 0.01944327 0.02450181\n",
      " 0.01924687 0.01546757 0.02128425 0.02193211 0.01697283 0.02128877\n",
      " 0.01313342 0.01669237 0.01878238 0.01687937 0.01810507 0.01370871\n",
      " 0.01987845 0.01739143 0.0188323  0.0222728  0.0167414  0.01366051\n",
      " 0.01756926 0.02310276 0.01789493 0.02054603 0.01854707 0.01928754\n",
      " 0.02050631 0.02324351 0.0228734  0.02051593 0.02232585 0.01997143\n",
      " 0.01822573 0.0163727  0.01718055 0.02095092 0.01492758 0.02603756\n",
      " 0.01489773 0.01501664 0.02572366 0.01866632 0.01605954 0.0246574\n",
      " 0.01744116 0.01600786 0.01925057 0.019717   0.01873595 0.0198426\n",
      " 0.01574089 0.01938001 0.01818747 0.02089597 0.0135618  0.02409905\n",
      " 0.01567228 0.0186342  0.01974329 0.02562355 0.01626231 0.02165303\n",
      " 0.01740792 0.01834391 0.01220706 0.02410379 0.02455011 0.01715242\n",
      " 0.01609051 0.01462406 0.0182958  0.01679442 0.02047455 0.02729751\n",
      " 0.01952948 0.01672727 0.01616516 0.02391446 0.02607447 0.02052605\n",
      " 0.02358942 0.02042596 0.02161413 0.01954937 0.01961023 0.02249345\n",
      " 0.01965814 0.01604101 0.02403329 0.01775287 0.01780392 0.01814098\n",
      " 0.01769439 0.01474882 0.01279542 0.02221208 0.01815957 0.01912744\n",
      " 0.01708622 0.01698173 0.02126616 0.02253724 0.02080622 0.01723817\n",
      " 0.02054361 0.01687451 0.02114982 0.01676543 0.01612136 0.01797512\n",
      " 0.01912919 0.01174333 0.01951975 0.01913319 0.01764075 0.01918521\n",
      " 0.02327379 0.01564129 0.0230031  0.01770961 0.02157325 0.02320527\n",
      " 0.02056484 0.02165403 0.02163413 0.01557093 0.01896674 0.01896835\n",
      " 0.01246885 0.01438378 0.02249289 0.01929077 0.01605334 0.02331499\n",
      " 0.01871391 0.02228344 0.01581273 0.02170985 0.02028583 0.02067872\n",
      " 0.02389073 0.01659248 0.01751756 0.01715585 0.02222084 0.0176973\n",
      " 0.02121305 0.01587468 0.01694676 0.02316822 0.02038994 0.01612412\n",
      " 0.01358216 0.01822076 0.0206084  0.02050709 0.02029005 0.02045494\n",
      " 0.02117249 0.02170124 0.02535003 0.01657766 0.01406822 0.02068044\n",
      " 0.02348783 0.02281086 0.01526987 0.01897068 0.02262822 0.02361207\n",
      " 0.01736696 0.02057121 0.0221147  0.01927259 0.01641838 0.01413219\n",
      " 0.01744871 0.01486034 0.02153311 0.01472805 0.01692081 0.02337151\n",
      " 0.02549963 0.01912783 0.02104483 0.01587366 0.01929553 0.02016951\n",
      " 0.01816789 0.01591561 0.02046245 0.0217633  0.01173811 0.01941091\n",
      " 0.01532807 0.01454146 0.01947878 0.0197068  0.02356761 0.02348829\n",
      " 0.0163045  0.01865868 0.02381415 0.02432026 0.02180343 0.014851\n",
      " 0.01885494 0.02435697 0.01949363 0.01938567 0.02161359 0.02388484\n",
      " 0.01990203 0.01621982 0.02185517 0.01521965 0.01757893 0.01578084\n",
      " 0.02358677 0.01870127 0.01171307 0.02223887 0.01813474 0.01800396\n",
      " 0.02570149 0.01603375 0.01448635 0.01229935 0.01542269 0.01808273\n",
      " 0.01724382 0.02007192 0.01943924 0.0153502  0.01487456 0.01895293\n",
      " 0.01348977 0.0142707  0.01557614 0.01836308 0.01530499 0.01667591\n",
      " 0.02371672 0.02093558 0.02214297 0.01954942 0.01614995 0.01612276\n",
      " 0.02187719 0.01639159 0.02426508 0.01666922 0.01310457 0.01516772\n",
      " 0.01535912 0.01920471 0.02116268 0.01758259 0.02320187 0.02422347\n",
      " 0.01952026 0.01682676 0.02166364 0.02405644 0.01756546 0.02188487\n",
      " 0.01833922 0.01831039 0.01525539 0.02281777 0.01804604 0.02233012\n",
      " 0.01610702 0.02593808 0.02364898 0.01567106 0.02179305 0.01661155\n",
      " 0.02274804 0.02527874 0.01894618 0.02036578 0.02115161 0.01536092\n",
      " 0.02062158 0.02364804 0.01758681 0.01962988 0.01856575 0.02211339\n",
      " 0.01830645 0.01849775 0.01932003 0.01952944 0.0239744  0.01827886\n",
      " 0.020661   0.01890035 0.02275513 0.02010342 0.02315568 0.02413232\n",
      " 0.01932326 0.0179116  0.02799438 0.02174903 0.02027311 0.01513064\n",
      " 0.01529118 0.0173475  0.02195963 0.01766765 0.01664815 0.01611336\n",
      " 0.01574549 0.02480189 0.02313165 0.02189805 0.02125661 0.01790215\n",
      " 0.01736799 0.01950307 0.01730069 0.02040172 0.02158476 0.02337635\n",
      " 0.01757514 0.01515035 0.0195114  0.02437017 0.01757939 0.01549008\n",
      " 0.01987844 0.02214437 0.02042768 0.01935359 0.02341143 0.02045652\n",
      " 0.02486559 0.0185454  0.02056837 0.01798028 0.01658181 0.02213391\n",
      " 0.01816696 0.0174919  0.02577444 0.02199372 0.01685647 0.01704622\n",
      " 0.02266914 0.02218831 0.01964895 0.01512869 0.02035829 0.01843973\n",
      " 0.02104149 0.02195949 0.02037168 0.01647575 0.0242685  0.01301968\n",
      " 0.01907728 0.01963813 0.0205175  0.01761639 0.02518111 0.01950797\n",
      " 0.01750434 0.0175249  0.01692599 0.01896692 0.01964042 0.02286298\n",
      " 0.01884744 0.02411248 0.01983748 0.02261497 0.02127621 0.02136456\n",
      " 0.01418539 0.02124325 0.01940435 0.012996   0.01753264 0.0172522\n",
      " 0.02262185 0.01594995 0.02405886 0.01412298 0.01486301 0.01813064\n",
      " 0.0190583  0.02717846 0.01654434 0.02203079 0.01552878 0.01506294\n",
      " 0.0210024  0.01666567 0.01851653 0.01888922 0.01994837 0.02116569\n",
      " 0.02087824 0.02230208 0.01901889 0.01634263 0.0219436  0.01890086\n",
      " 0.02171678 0.02006103 0.02229736 0.01968156 0.02327513 0.02077981\n",
      " 0.01389045 0.02239485 0.02419738 0.01743072 0.02659493 0.02107529\n",
      " 0.02299963 0.01759701 0.02182712 0.01307273 0.02459013 0.0200352\n",
      " 0.02225688 0.0244427  0.02339049 0.02129766 0.02316011 0.01806271\n",
      " 0.0209963  0.0203002  0.02369308 0.02374195 0.01545612 0.01279988\n",
      " 0.01913701 0.02445516 0.02228864 0.02220766 0.02215864 0.02170456\n",
      " 0.01929882 0.01844585 0.0174845  0.02104768 0.01678447 0.02498149\n",
      " 0.02013545 0.01682361 0.01845782 0.02144082 0.0224972  0.01395467\n",
      " 0.02215693 0.02184569 0.02895813 0.01850389 0.02205612 0.01911001\n",
      " 0.01816083 0.01685883 0.01743287 0.02289358 0.02226395 0.01856579\n",
      " 0.02197236 0.01930953 0.02027279 0.02351136 0.0236925  0.02112729\n",
      " 0.02409769 0.02186334 0.0226194  0.01981501 0.02092075 0.01365712\n",
      " 0.01862778 0.01745224 0.01819324 0.02265221 0.02333421 0.01411566\n",
      " 0.0212709  0.01752997 0.01528841 0.01838234 0.01717215 0.02025635\n",
      " 0.019576   0.01591102 0.01680154 0.0234039  0.01487436 0.01574204\n",
      " 0.01863886 0.01778786 0.01587486 0.02120251 0.01768427 0.02525974\n",
      " 0.02021347 0.02185893 0.01574079 0.02520875 0.0182795  0.01589556\n",
      " 0.0236368  0.01660933 0.01599185 0.01713537]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "inn = queries[3][0].dot(norm_rand_vectors.transpose())\n",
    "h = []\n",
    "for i in inn:\n",
    "    if i>0.03:\n",
    "        h.append(1)\n",
    "    else:\n",
    "        h.append(0)\n",
    "print(inn)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9214119599831555"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(query_projection[0],random_projections[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_random_projections()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
