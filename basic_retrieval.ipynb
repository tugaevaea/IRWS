{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import copy\n",
    "import numpy as np \n",
    "import itertools\n",
    "import more_itertools as mit\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer \n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Term frequency\n",
    "def tf(corpus, column_name):\n",
    "    \n",
    "    def tokenize(string):\n",
    "        return string.split()\n",
    "    \n",
    "    def tf_string(string): \n",
    "        # create bag of words from the string\n",
    "        bow = tokenize(string)\n",
    "    \n",
    "        tf_dict = {}\n",
    "        for word in bow:\n",
    "            if word in tf_dict:\n",
    "                tf_dict[word] += 1\n",
    "            else:\n",
    "                tf_dict[word] = 1\n",
    "            \n",
    "        for word in tf_dict:\n",
    "            tf_dict[word] = tf_dict[word]/len(bow)### ??\n",
    "    \n",
    "        return tf_dict\n",
    "    \n",
    "    # call our function on every doc and store all these tf dictionaries. \n",
    "    tf_dict = {}\n",
    "    for index, row in corpus.iterrows():\n",
    "        doc_dict = tf_string(row[column_name])\n",
    "        tf_dict[index] = doc_dict\n",
    "            \n",
    "    return tf_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inveresed document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inversed document frequency\n",
    "def idf(corpus, tf_dict):\n",
    "    \n",
    "    # nomber of documents in corpus\n",
    "    no_of_docs = len(corpus.index)\n",
    "    \n",
    "    # term - key, number of docs term occured in\n",
    "    def count_occurances(tf_dict):\n",
    "        count_dict = {}\n",
    "        for key in tf_dict:\n",
    "            for key in tf_dict[key]:\n",
    "                if key in count_dict:\n",
    "                    count_dict[key] += 1\n",
    "                else:\n",
    "                    count_dict[key] = 1\n",
    "        return count_dict\n",
    "\n",
    "    idf_dict = {}\n",
    "    \n",
    "    count_dict = count_occurances(tf_dict)\n",
    "    \n",
    "    for key in count_dict:\n",
    "        idf_dict[key] = math.log(no_of_docs/count_dict[key])\n",
    "    \n",
    "    return idf_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "def tf_idf(tf_dict, idf_dict):   \n",
    "    tf_idf_dict = copy.deepcopy(tf_dict)\n",
    "    for doc, value in tf_idf_dict.items():\n",
    "        for word, value in tf_idf_dict[doc].items():\n",
    "            tf_idf_dict[doc][word] = value * idf_dict[word]\n",
    "    return tf_idf_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity\n",
    "def cosine_similarity(v1, v2):\n",
    "    \n",
    "    def vector_magnitude(v):\n",
    "        return np.linalg.norm(v)\n",
    "    \n",
    "    def dot_product(v1, v2):\n",
    "        return np.dot(v1,v2)\n",
    "    \n",
    "    return dot_product(v1, v2)/ (vector_magnitude(v1) * vector_magnitude(v2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF to matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tf_idf_dict to matrix\n",
    "def tf_idf_to_matrix(tf_idf_dict):\n",
    "    tf_idf_matrix = pd.DataFrame.from_dict(tf_idf_dict, \n",
    "                                           orient = 'index').fillna(0) # if word does not appear in doc we change NaN to\n",
    "    return tf_idf_matrix.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF for queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_query(query):\n",
    "    if type(query) == str:\n",
    "        tokenized_query = query.split()\n",
    "    else:\n",
    "        tokenized_query = query\n",
    "\n",
    "    df_query = tf_idf_matrix[0:0]  # dataframe of tf-idf weights of a query\n",
    "    df_query = df_query.append(pd.Series(0, index=df_query.columns), ignore_index=True)\n",
    "    for token in tokenized_query:\n",
    "        for col in df_query.columns:\n",
    "            if token == col:\n",
    "                df_query[col][0] = df_query[col][0] + 1  # raw term frequency\n",
    "\n",
    "    df_query = df_query.replace(0, np.nan)\n",
    "\n",
    "    df_query = np.log(df_query) + 1  # log term freq(as in the slides)\n",
    "\n",
    "    df_query = df_query.fillna(0)\n",
    "\n",
    "    for col in df_query.columns:\n",
    "        df_query[col][0] = df_query[col][0] * idf_dict[col]\n",
    "\n",
    "    return df_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_retrieve(corpus, queries, query_index, top_k, random_projections=False):\n",
    "    \"\"\"\n",
    "    Retrieve top relevant document for input query.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus:          dataframe with documents\n",
    "    column_name:     column name (string) of corpus dataframe with document content\n",
    "    query_index:     index a query in the query Series\n",
    "    tf_idf_queries:  dataframe with tf-idf scores for all the queries\n",
    "    top_k:           number of most relevant documents to be output\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df.iloc[ids]     dataframe with IDs of predicted top_k most relevant documents with their content\n",
    "    \"\"\"\n",
    "    \n",
    "    query = queries['TEXT'][query_index]\n",
    "        \n",
    "    tf_dict = tf(corpus, column_name = 'TEXT')                   # tf for docs\n",
    "    idf_dict = idf(corpus, tf_dict)                              # idf for docs\n",
    "    tf_idf_dict = tf_idf(tf_dict, idf_dict)                      # tf-idf for docs\n",
    "    tf_idf_matrix = tf_idf_to_matrix(tf_idf_dict)                # tf-idf dictionary for docs converted to matrix\n",
    "    tf_idf_queries = vectorize_query(query)                      # tf-idf for input query\n",
    "    doc_vectors = tf_idf_matrix.values                           \n",
    "    q_vectors = tf_idf_queries.values\n",
    "    \n",
    "    df = corpus.copy()\n",
    "\n",
    "    sim = []                                                     # to store cosine similarities\n",
    "    sort_sim = []                                                # sorted cosine similarities\n",
    "    i = 0\n",
    "    for doc in doc_vectors:\n",
    "        #in case of random projections calculate dot product since our document and query vectors would be unit-normalized\n",
    "        if random_projections:\n",
    "            sim.append([i, np.dot(q_vectors[query_index], doc)])\n",
    "        else:\n",
    "            sim.append([i, cosine_similarity(q_vectors[query_index], doc)])\n",
    "        i += 1\n",
    "    sort_sim = sorted(sim, key=lambda cos: cos[1], reverse=True)\n",
    "    ids = []\n",
    "\n",
    "    for j in range(top_k):\n",
    "        ids.append(sort_sim[j][0])\n",
    "\n",
    "    return df.iloc[ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST (find 5 most relevant documents for first query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load corpus \n",
    "# corpus = pd.read_csv('nfcorpus/dev.docs', sep='\\t', names=['ID', 'TEXT'])\n",
    "\n",
    "# load queries\n",
    "# queries = pd.read_csv('nfcorpus/dev.all.queries', sep='\\t', names=['ID', 'TEXT'])\n",
    "\n",
    "#test = basic_retrieve(corpus = corpus, \n",
    "#                      queries = queries,\n",
    "#                      query_index = 0,\n",
    "#                      top_k = 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
